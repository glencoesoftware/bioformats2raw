/**
 * Copyright (c) 2019 Glencoe Software, Inc. All rights reserved.
 *
 * This software is distributed under the terms described by the LICENSE.txt
 * file you can find at the root of the distribution bundle.  If the file is
 * missing please request a copy by contacting info@glencoesoftware.com
 */
package com.glencoesoftware.bioformats2raw;

import java.io.File;
import java.io.IOException;
import java.io.ByteArrayOutputStream;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.file.FileSystem;
import java.nio.file.FileSystems;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.IntStream;

import loci.common.Constants;
import loci.common.Region;
import loci.common.image.IImageScaler;
import loci.common.image.SimpleImageScaler;
import loci.common.services.DependencyException;
import loci.common.services.ServiceException;
import loci.common.services.ServiceFactory;
import loci.formats.ChannelSeparator;
import loci.formats.ClassList;
import loci.formats.FormatException;
import loci.formats.FormatTools;
import loci.formats.IFormatReader;
import loci.formats.ImageReader;
import loci.formats.Memoizer;
import loci.formats.MinMaxCalculator;
import loci.formats.MissingLibraryException;
import loci.formats.in.DynamicMetadataOptions;
import loci.formats.meta.IMetadata;
import loci.formats.ome.OMEXMLMetadata;
import loci.formats.services.OMEXMLService;
import loci.formats.services.OMEXMLServiceImpl;
import ome.units.quantity.Length;
import ome.units.quantity.Quantity;
import ome.units.quantity.Time;
import ome.xml.meta.OMEXMLMetadataRoot;
import ome.xml.model.Channel;
import ome.xml.model.Filter;
import ome.xml.model.FilterSet;
import ome.xml.model.Image;
import ome.xml.model.Laser;
import ome.xml.model.LightPath;
import ome.xml.model.LightSource;
import ome.xml.model.LightSourceSettings;
import ome.xml.model.TransmittanceRange;
import ome.xml.model.enums.DimensionOrder;
import ome.xml.model.enums.EnumerationException;
import ome.xml.model.enums.UnitsLength;
import ome.xml.model.enums.UnitsTime;
import ome.xml.model.primitives.Color;
import ome.xml.model.primitives.PositiveInteger;

import org.perf4j.slf4j.Slf4JStopWatch;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.glencoesoftware.bioformats2raw.MiraxReader.TilePointer;
import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import com.google.common.math.DoubleMath;
import com.univocity.parsers.csv.CsvParser;
import com.univocity.parsers.csv.CsvParserSettings;

import ch.qos.logback.classic.Level;
import picocli.CommandLine;
import picocli.CommandLine.Option;
import picocli.CommandLine.Parameters;
import ucar.ma2.InvalidRangeException;

// jzarr: v2

import com.bc.zarr.ArrayParams;
import com.bc.zarr.CompressorFactory;
import com.bc.zarr.DataType;
import com.bc.zarr.DimensionSeparator;
import com.bc.zarr.ZarrArray;
import com.bc.zarr.ZarrGroup;

// zarr-java: v3

import dev.zarr.zarrjava.ZarrException;
import dev.zarr.zarrjava.store.FilesystemStore;
import dev.zarr.zarrjava.store.StoreHandle;
import dev.zarr.zarrjava.utils.Utils;
import dev.zarr.zarrjava.v3.Array;
import dev.zarr.zarrjava.v3.ArrayMetadata;
import dev.zarr.zarrjava.v3.Group;
import dev.zarr.zarrjava.v3.codec.Codec;
import dev.zarr.zarrjava.v3.codec.CodecBuilder;
import dev.zarr.zarrjava.v3.codec.core.ShardingIndexedCodec;

/**
 * Command line tool for converting whole slide imaging files to Zarr.
 */
public class Converter implements Callable<Integer> {

  private static final Logger LOGGER = LoggerFactory.getLogger(Converter.class);

  /**
   * Relative path to OME-XML metadata file.
   */
  private static final String METADATA_FILE = "METADATA.ome.xml";

  /**
   * Minimum size of the largest XY dimension in the smallest resolution,
   * when calculating the number of resolutions to generate.
   */
  private static final int MIN_SIZE = 256;

  /** Scaling factor in X and Y between any two consecutive resolutions. */
  private static final int PYRAMID_SCALE = 2;

  /** Version of the bioformats2raw layout. */
  public static final Integer LAYOUT = 3;

  /** NGFF specification version.*/
  public static final String NGFF_VERSION = "0.4";
  public static final String NGFF_VERSION_V3 = "0.5";

  private volatile Path inputPath;
  private volatile String outputLocation;

  private volatile boolean warnNoExec = false;

  private Map<String, String> outputOptions;
  private volatile Integer pyramidResolutions;
  private volatile List<Integer> seriesList;

  private volatile int tileWidth;
  private volatile int tileHeight;
  private volatile int chunkDepth;

  private volatile int shardWidth;
  private volatile int shardHeight;
  private volatile int shardDepth;

  private volatile String logLevel;
  private volatile boolean progressBars = false;
  private volatile boolean printVersion = false;
  private volatile boolean help = false;
  private volatile boolean originalMetadata = true;

  private volatile boolean v3 = false;
  private volatile FilesystemStore v3Store = null;

  private volatile int maxWorkers;
  private volatile int maxCachedTiles;
  private volatile ZarrCompression compressionType;
  private volatile Map<String, Object> compressionProperties;
  private volatile boolean compressInnerChunk = false;
  private volatile Class<?>[] extraReaders;
  private volatile boolean omeroMetadata = true;
  private volatile boolean nested = true;
  private volatile String pyramidName = null;
  private volatile String scaleFormatString;
  private volatile Path additionalScaleFormatStringArgsCsv;

  /** Additional scale format string arguments after parsing. */
  private volatile List<String[]> additionalScaleFormatStringArgs;

  private volatile DimensionOrder dimensionOrder;
  private volatile File memoDirectory;
  private volatile boolean keepMemoFiles = false;
  private volatile Downsampling downsampling;
  private volatile boolean overwrite = false;
  private volatile Short fillValue = null;
  private volatile List<String> readerOptions;
  private volatile boolean noHCS = false;
  private volatile boolean noOMEMeta = false;
  private volatile boolean noRootGroup = false;
  private volatile boolean reuseExistingResolutions = false;
  private volatile int minSize;
  private volatile boolean writeImageData = true;
  private volatile boolean compactDims = false;

  /** Scaling implementation that will be used during downsampling. */
  private volatile IImageScaler scaler = new SimpleImageScaler();

  /**
   * Set of readers that can be used concurrently, size will be equal to
   * {@link #maxWorkers}.
   */
  private volatile BlockingQueue<IFormatReader> readers;

  /**
   * Bounded task queue limiting the number of in flight conversion operations
   * happening in parallel.  Size will be equal to {@link #maxWorkers}.
   */
  private volatile BlockingQueue<Runnable> queue;

  private volatile ExecutorService executor;

  /**
   * The source file's pixel type.  Retrieved from
   * {@link IFormatReader#getPixelType()}.
   */
  private volatile int pixelType;

  /** Total number of tiles at the current resolution during processing. */
  private volatile int tileCount;

  /** Current number of tiles processed at the current resolution. */
  private volatile AtomicInteger nTile;

  private List<HCSIndex> hcsIndexes = new ArrayList<HCSIndex>();

  /** Calculated from outputLocation. */
  private volatile Path outputPath;

  private IProgressListener progressListener;
  private Map<Integer, int[]> tileCounts = new HashMap<Integer, int[]>();

  // Option setters

  /**
   * @param input path to the input data
   */
  @Parameters(
    index = "0",
    arity = "1",
    description = "file to convert",
    defaultValue = Option.NULL_VALUE
  )
  public void setInputPath(String input) {
    if (input != null) {
      // this could be expanded later to support files not on disk
      inputPath = Paths.get(input);
    }
    else {
      inputPath = null;
    }
  }

  /**
   * @param output path or URI where output data should be written
   */
  @Parameters(
    index = "1",
    arity = "1",
    description = "path to the output pyramid directory. " +
      "The given path can also be a URI (containing ://) " +
      "which will activate **experimental** support for " +
      "Filesystems. For example, if the output path given " +
      "is 's3://my-bucket/some-path' *and* you have an "+
      "S3FileSystem implementation in your classpath, then " +
      "all files will be written to S3.",
    defaultValue = Option.NULL_VALUE
  )
  public void setOutputPath(String output) {
    outputLocation = output;
  }

  /**
   * Options to use when connecting to a remote filesystem, e.g. s3.
   *
   * @param options output filesystem options
   */
  @Option(
    names = "--output-options",
    split = "\\|",
    description = "|-separated list of key-value pairs " +
      "to be used as an additional argument to Filesystem " +
      "implementations if used. For example, " +
      "--output-options=s3fs_path_style_access=true|... " +
      "might be useful for connecting to minio.",
    defaultValue = Option.NULL_VALUE
  )
  public void setOutputOptions(Map<String, String> options) {
    outputOptions = options;
  }

  /**
   * Define the number of resolutions in the generated pyramid.
   * By default, the resolution count is calculated based upon
   * the input image size. The resolution count includes the
   * full-resolution image. If the resolution count is null,
   * then the number of resolutions will be calculated automatically.
   * If not null, it must be greater than 0.
   *
   * @param resolutions pyramid resolution count
   */
  @Option(
    names = {"-r", "--resolutions"},
    description = "Number of pyramid resolutions to generate",
    defaultValue = Option.NULL_VALUE
  )
  public void setResolutions(Integer resolutions) {
    if (resolutions == null || resolutions > 0) {
      pyramidResolutions = resolutions;
    }
    else {
      LOGGER.warn("Ignoring invalid resolution count: {}", resolutions);
    }
  }

  /**
   * Define a subset of input series indexes to convert.
   * Series indexes begin at 0 and are the Bio-Formats series/OME Image
   * index reported when resolution flattening has not been applied.
   *
   * @param seriesToConvert series index list
   */
  @Option(
    names = {"-s", "--series"},
    arity = "0..1",
    split = ",",
    description = "Comma-separated list of series indexes to convert",
    defaultValue = Option.NULL_VALUE
  )
  public void setSeriesList(List<Integer> seriesToConvert) {
    if (seriesToConvert != null) {
      seriesList = seriesToConvert;
    }
    else {
      seriesList = new ArrayList<Integer>();
    }
  }

  /**
   * Set the maximum tile width (X chunk size) for input and output.
   *
   * @param width tile width
   */
  @Option(
    names = {"-w", "--tile-width", "--tile_width"},
    description = "Maximum tile width (default: ${DEFAULT-VALUE}). " +
      "This is both the chunk size (in X) when writing Zarr and the tile " +
      "size used for reading from the original data. Changing the tile " +
      "size may have performance implications.",
    defaultValue = "1024"
  )
  public void setTileWidth(int width) {
    if (width > 0) {
      tileWidth = width;
    }
    else {
      LOGGER.warn("Ignoring invalid tile width: {}", width);
    }
  }

  /**
   * Set the maximum tile height (Y chunk size) for input and output.
   *
   * @param height tile height
   */
  @Option(
    names = {"-h", "--tile-height", "--tile_height"},
    description = "Maximum tile height (default: ${DEFAULT-VALUE}). " +
      "This is both the chunk size (in Y) when writing Zarr and the tile " +
      "size used for reading from the original data. Changing the tile " +
      "size may have performance implications.",
    defaultValue = "1024"
  )
  public void setTileHeight(int height) {
    if (height > 0) {
      tileHeight = height;
    }
    else {
      LOGGER.warn("Ignoring invalid tile height: {}", height);
    }
  }

  /**
   * Set the maximum depth (Z chunk size) for writing Zarr.
   *
   * @param depth Z chunk size
   */
  @Option(
    names = {"-z", "--chunk-depth", "--chunk_depth"},
    description = "Maximum chunk depth to read (default: ${DEFAULT-VALUE}) ",
    defaultValue = "1"
  )
  public void setChunkDepth(int depth) {
    if (depth > 0) {
      chunkDepth = depth;
    }
    else {
      LOGGER.warn("Ignoring invalid chunk depth: {}", depth);
    }
  }

  /**
   * Set the maximum shard width (X shard size) when writing v3.
   *
   * @param width shard width
   */
  @Option(
    names = {"--shard-width", "--shard_width"},
    description = "Maximum shard width (default: ${DEFAULT-VALUE}). " +
      "Changing this may have performance implications.",
    defaultValue = "1024"
  )
  public void setShardWidth(int width) {
    if (width > 0) {
      shardWidth = width;
    }
    else {
      LOGGER.warn("Ignoring invalid shard width: {}", width);
    }
  }

  /**
   * Set the maximum shard height (Y shard size) when writing v3.
   *
   * @param height shard height
   */
  @Option(
    names = {"--shard-height", "--shard_height"},
    description = "Maximum shard height (default: ${DEFAULT-VALUE}). " +
      "Changing this may have performance implications.",
    defaultValue = "1024"
  )
  public void setShardHeight(int height) {
    if (height > 0) {
      shardHeight = height;
    }
    else {
      LOGGER.warn("Ignoring invalid shard height: {}", height);
    }
  }

  /**
   * Set the maximum shard depth (Z shard size) when writing v3.
   *
   * @param depth Z shard size
   */
  @Option(
    names = {"--shard-depth", "--shard_depth"},
    description = "Maximum shard depth to read (default: ${DEFAULT-VALUE}) ",
    defaultValue = "1"
  )
  public void setShardDepth(int depth) {
    if (depth > 0) {
      shardDepth = depth;
    }
    else {
      LOGGER.warn("Ignoring invalid shard depth: {}", depth);
    }
  }

  /**
   * Set whether or not tiles should actually be converted.
   *
   * @param noTiles true if tiles should not be converted
   */
  @Option(
    names = {"--no-tiles"},
    description = "Write all metadata, but do not convert any tiles",
    defaultValue = "false"
  )
  public void setNoTiles(boolean noTiles) {
    writeImageData = !noTiles;
  }

  /**
   * Set the slf4j logging level. Defaults to "WARN".
   *
   * @param level logging level
   */
  @Option(
    names = {"--log-level", "--debug"},
    arity = "0..1",
    description = "Change logging level; valid values are " +
      "OFF, ERROR, WARN, INFO, DEBUG, TRACE and ALL. " +
      "(default: ${DEFAULT-VALUE})",
    defaultValue = "WARN",
    fallbackValue = "DEBUG"
  )
  public void setLogLevel(String level) {
    if (level != null) {
      logLevel = level;
    }
  }

  /**
   * Configure whether or not progress bars are shown during conversion.
   * Progress bars are turned off by default.
   *
   * @param useProgressBars whether or not to show progress bars
   */
  @Option(
    names = {"-p", "--progress"},
    description = "Print progress bars during conversion",
    defaultValue = "false"
  )
  public void setProgressBars(boolean useProgressBars) {
    progressBars = useProgressBars;
  }

  /**
   * Configure whether to warn instead of throwing an exception
   * if files created in the temporary directory
   * ("java.io.tmpdir" system property) will not be executable.
   *
   * @param warnOnly true if a warning should be logged instead of an exception
   */
  @Option(
    names = {"--warn-no-exec"},
    description = "Warn instead of throwing an exception if " +
                  "java.io.tmpdir is not executable",
    defaultValue = "false"
  )
  public void setWarnOnNoExec(boolean warnOnly) {
    warnNoExec = warnOnly;
  }

  /**
   * Configure whether to print version information and exit
   * without converting.
   *
   * @param versionOnly whether or not to print version information and exit
   */
  @Option(
    names = "--version",
    description = "Print version information and exit",
    help = true,
    defaultValue = "false"
  )
  public void setPrintVersionOnly(boolean versionOnly) {
    printVersion = versionOnly;
  }

  /**
   * Configure whether to print help and exit without converting.
   *
   * @param helpOnly whether or not to print help and exit
   */
  @Option(
    names = "--help",
    description = "Print usage information and exit",
    usageHelp = true,
    defaultValue = "false"
  )
  public void setHelp(boolean helpOnly) {
    help = helpOnly;
  }

  /**
   * Set the maximum number of workers to use for converting tiles.
   * Defaults to 4 or the number of detected CPUs, whichever is smaller.
   *
   * @param workers maximum worker count
   */
  @Option(
    names = {"--max-workers", "--max_workers"},
    description = "Maximum number of workers (default: ${DEFAULT-VALUE})",
    defaultValue = "4"
  )
  public void setMaxWorkers(int workers) {
    int availableProcessors = Runtime.getRuntime().availableProcessors();
    if (workers > availableProcessors) {
      maxWorkers = availableProcessors;
    }
    else if (workers > 0) {
      maxWorkers = workers;
    }
    else {
      LOGGER.warn("Ignoring invalid worker count: {}", workers);
    }
  }

  /**
   * Set the maximum number of tiles that can be cached.
   * Depending upon the input format, this can improve conversion performance
   * by reducing the number of tile reads.
   *
   * @param maxTiles maximum number of cached tiles
   */
  @Option(
    names = {"--max-cached-tiles", "--max_cached_tiles"},
    description =
      "Maximum number of tiles that will be cached across all "
      + "workers (default: ${DEFAULT-VALUE})",
    defaultValue = "64"
  )
  public void setMaxCachedTiles(int maxTiles) {
    maxCachedTiles = maxTiles;
  }

  /**
   * Set the compression type for the output Zarr. Defaults to blosc.
   *
   * @param compression compression type
   */
  @Option(
          names = {"-c", "--compression"},
          description = "Compression type for Zarr " +
                  "(${COMPLETION-CANDIDATES}; default: ${DEFAULT-VALUE})",
          defaultValue = "blosc"
  )
  public void setCompression(ZarrCompression compression) {
    if (compression != null) {
      compressionType = compression;
    }
  }

  /**
   * Compression-specific options as defined by jzarr.
   *
   * @param properties compression properties
   */
  @Option(
          names = {"--compression-properties"},
          description = "Properties for the chosen compression (see " +
            "https://jzarr.readthedocs.io/en/latest/tutorial.html#compressors" +
            " )",
          defaultValue = Option.NULL_VALUE
  )
  public void setCompressionProperties(Map<String, Object> properties) {
    if (properties != null) {
      compressionProperties = properties;
    }
    else {
      compressionProperties = new HashMap<String, Object>();
    }
  }

  /**
   * Set whether compression applies to inner chunk when sharding is used.
   * By default, compression is applied to the shard, not the chunks
   * within the shard.
   *
   * @param compressChunk true if the chunks within a shard are compressed
   */
  @Option(
      names = {"--compress-inner-chunk"},
      description = "True if compression options apply to chunks in a shard",
      defaultValue = "false"
  )
  public void setCompressInnerChunk(boolean compressChunk) {
    compressInnerChunk = compressChunk;
  }

  /**
   * List of extra readers to use. This can include any reader that is on
   * the classpath and not defined by Bio-Formats. By default this includes
   * every reader in the bioformats2raw repository.
   *
   * @param extraReaderList list of extra readers
   */
  @Option(
          names = "--extra-readers",
          arity = "0..1",
          split = ",",
          description = "Separate set of readers to include; " +
                  "(default: ${DEFAULT-VALUE})",
          defaultValue =
            "com.glencoesoftware.bioformats2raw.PyramidTiffReader," +
            "com.glencoesoftware.bioformats2raw.MiraxReader," +
            "com.glencoesoftware.bioformats2raw.BioTekReader," +
            "com.glencoesoftware.bioformats2raw.ND2PlateReader," +
            "com.glencoesoftware.bioformats2raw.MetaxpressReader," +
            "com.glencoesoftware.bioformats2raw.MCDReader," +
            "com.glencoesoftware.bioformats2raw.PhenixReader"
  )
  public void setExtraReaders(Class<?>[] extraReaderList) {
    if (extraReaderList != null) {
      extraReaders = extraReaderList;
    }
  }

  /**
   * Configure whether or not to calculate min/max pixel values and
   * write OMERO rendering metadata. By default, min/max values are
   * calculated and rendering metadata is written.
   *
   * @param noMinMax false if OMERO rendering metadata should be written
   */
  @Option(
          names = "--no-minmax", negatable=true,
          description = "Whether to calculate minimum and maximum pixel " +
                        "values. Min/max calculation can result in slower " +
                        "conversions. If true, min/max values are saved as " +
                        "OMERO rendering metadata (true by default)",
          defaultValue = "false"
  )
  public void setCalculateOMEROMetadata(boolean noMinMax) {
    omeroMetadata = !noMinMax;
  }

  /**
   * Configure whether or not the output Zarr is written as nested,
   * using the '/' chunk separator.
   *
   * @param unnested false if nested chunk storage should be used
   */
  @Option(
          names = "--no-nested", negatable=true,
          description = "Whether to use '/' as the chunk path separator " +
                  "(true by default)",
          defaultValue = "false"
  )
  public void setUnnested(boolean unnested) {
    nested = !unnested;
  }

  /**
   * Set the name of the pyramid being converted.
   * If not null, this will be used to create a subdirectory
   * in the output location.
   * By default, null for compatibility with raw2ometiff.
   *
   * @param pyramid pyramid name
   */
  @Option(
          names = "--pyramid-name",
          description = "Name of pyramid (default: ${DEFAULT-VALUE}) " +
                  "[Can break compatibility with raw2ometiff]",
          defaultValue = Option.NULL_VALUE
  )
  public void setPyramidName(String pyramid) {
    pyramidName = pyramid;
  }

  /**
   * Set the format string for scale paths.
   *
   * @param formatString scale path format string
   */
  @Option(
          names = "--scale-format-string",
          description = "Format string for scale paths; the first two " +
                  "arguments will always be series and resolution followed " +
                  "by any additional arguments brought in from " +
                  "`--additional-scale-format-string-args` " +
                  "[Can break compatibility with raw2ometiff] " +
                  "(default: ${DEFAULT-VALUE})",
          defaultValue = "%d/%d"
  )
  public void setScaleFormat(String formatString) {
    if (formatString != null) {
      scaleFormatString = formatString;
    }
  }

  /**
   * Set path to a CSV file with additional scale formatting arguments.
   *
   * @param scaleFormatCSV path to CSV file
   */
  @Option(
          names = "--additional-scale-format-string-args",
          description = "Additional format string argument CSV file (without " +
                  "header row).  Arguments will be added to the end of the " +
                  "scale format string mapping the at the corresponding CSV " +
                  "row index.  It is expected that the CSV file contain " +
                  "exactly the same number of rows as the input file has " +
                  "series",
          defaultValue = Option.NULL_VALUE
  )
  public void setAdditionalScaleFormatCSV(Path scaleFormatCSV) {
    additionalScaleFormatStringArgsCsv = scaleFormatCSV;
  }

  /**
   * Set whether or not to use Zarr v3.
   * By default, v2 is used.
   *
   * @param useV3 true if Zarr v3 should be written
   */
  @Option(
          names = "--v3",
          description = "Write Zarr v3 data",
          defaultValue = "false"
  )
  public void setV3(boolean useV3) {
    v3 = useV3;
  }

  /**
   * Set the directory for memo (metadata cache) files.
   *
   * @param memoDir memo file directory
   */
  @Option(
          names = "--memo-directory",
          description = "Directory used to store .bfmemo cache files",
          defaultValue = Option.NULL_VALUE
  )
  public void setMemoDirectory(File memoDir) {
    memoDirectory = memoDir;
  }

  /**
   * Set whether or not to keep memo (metadata cache) files after conversion.
   * By default, memo files are deleted after conversion.
   *
   * @param keepMemos whether or not to keep memo files
   */
  @Option(
          names = "--keep-memo-files",
          description = "Do not delete .bfmemo files created during conversion",
          defaultValue = "false"
  )
  public void setKeepMemoFiles(boolean keepMemos) {
    keepMemoFiles = keepMemos;
  }

  /**
   * Set downsampling algorithm.
   *
   * @param downsampleType downsampling type
   */
  @Option(
          names = "--downsample-type",
          description = "Tile downsampling algorithm " +
            "(${COMPLETION-CANDIDATES})",
          defaultValue = "SIMPLE"
  )
  public void setDownsampling(Downsampling downsampleType) {
    if (downsampleType != null) {
      downsampling = downsampleType;
    }
  }

  /**
   * Set whether or not to overwrite an existing output directory.
   *
   * @param canOverwrite whether or not overwriting is allowed
   */
  @Option(
          names = "--overwrite",
          description = "Overwrite the output directory if it exists",
          defaultValue = "false"
  )
  public void setOverwrite(boolean canOverwrite) {
    overwrite = canOverwrite;
  }

  /**
   * Set the fill value for missing tiles. Must be in the range 0-255.
   * Currently applies to .mrxs input data only.
   *
   * @param tileFill pixel value (0-255) to fill
   */
  @Option(
          names = "--fill-value",
          description = "Default value to fill in for missing tiles (0-255)",
          defaultValue = Option.NULL_VALUE
  )
  public void setFillValue(Short tileFill) {
    if (tileFill == null || (tileFill >= 0 && tileFill <= 255)) {
      fillValue = tileFill;
    }
    else {
      LOGGER.warn("Ignoring invalid fill value (must be 0-255): {}", tileFill);
    }
  }

  /**
   * Set any reader-specific options.
   * See https://docs.openmicroscopy.org/bio-formats/latest/formats/options.html
   *
   * @param readerOpts reader options
   */
  @Option(
          arity = "0..1",
          names = "--options",
          split = ",",
          description =
            "Reader-specific options, in format key=value[,key2=value2]",
          defaultValue = Option.NULL_VALUE
  )
  public void setReaderOptions(List<String> readerOpts) {
    if (readerOpts != null) {
      readerOptions = readerOpts;
    }
    else {
      readerOptions = new ArrayList<String>();
    }
  }

  /**
   * Set whether or not to write HCS (plate/well) metadata.
   * See https://ngff.openmicroscopy.org/latest/#plate-md.
   * HCS metadata is written by default when the input data
   * represents a plate.
   *
   * @param noHCSWriting true if HCS metadata should not be written
   */
  @Option(
          names = "--no-hcs",
          description = "Turn off HCS writing",
          defaultValue = "false"
  )
  public void setNoHCS(boolean noHCSWriting) {
    noHCS = noHCSWriting;
  }

  /**
   * Set whether or not to write OME-XML metadata.
   * See https://ngff.openmicroscopy.org/latest/#bf2raw
   * By default, OME-XML metadata is written.
   *
   * @param noOMEMetaWriting true if OME-XML should not be written
   */
  @Option(
          names = "--no-ome-meta-export",
          description = "Turn off OME metadata exporting " +
                        "[Will break compatibility with raw2ometiff]",
          defaultValue = "false"
  )
  public void setNoOMEMeta(boolean noOMEMetaWriting) {
    noOMEMeta = noOMEMetaWriting;
  }

  /**
   * Set whether or not to write original metadata key/value pairs in
   * OME-XML metadata.
   * By default, original metadata is written.
   *
   * @param noOriginalMetadata true if original metadata should not be written
   */
  @Option(
          names = "--no-original-metadata",
          description = "Turn off original metadata exporting",
          defaultValue = "false"
  )
  public void setNoOriginalMetadata(boolean noOriginalMetadata) {
    originalMetadata = !noOriginalMetadata;
  }

  /**
   * Set whether or not to write a root Zarr group.
   * By default, the root group is written.
   *
   * @param noRootGroupWriting true if the root group should not be written
   */
  @Option(
          names = "--no-root-group",
          description = "Turn off creation of root group and corresponding " +
                        "metadata [Will break compatibility with raw2ometiff]",
          defaultValue = "false"
  )
  public void setNoRootGroup(boolean noRootGroupWriting) {
    noRootGroup = noRootGroupWriting;
  }

  /**
   * Set whether or not to keep pyramids from the input data.
   * By default, only the full-resolution images from the input data are kept.
   * Pyramid resolutions are generated by downsampling the full-resolution
   * image, and any pyramid resolutions in the input data are ignored.
   *
   * @param reuse whether or not to reuse pyramids in the input data, if present
   */
  @Option(
      names = "--use-existing-resolutions",
      description = "Use existing sub resolutions from original input format" +
          "[Will break compatibility with raw2ometiff]",
      defaultValue = "false"
  )
  public void setReuseExistingResolutions(boolean reuse) {
    reuseExistingResolutions = reuse;
  }

  /**
   * The target size of the largest XY dimension in the
   * smallest pyramid resolution. Defaults to 256.
   * Used to calculate the number of pyramid resolutions,
   * if the resolution count was not explicitly set.
   *
   * @param min target size of smallest pyramid resolution
   */
  @Option(
      names = "--target-min-size",
      description = "Specifies the desired size for the largest XY dimension " +
          "of the smallest resolution, when calculating the number " +
          "of resolutions generate. If the target size cannot be matched " +
          "exactly, the largest XY dimension of the smallest resolution " +
          "should be smaller than the target size.",
      defaultValue = "" + MIN_SIZE
  )
  public void setMinImageSize(int min) {
    if (min > 0) {
      minSize = min;
    }
    else {
      LOGGER.warn("Ignoring invalid minimum image size: {}", min);
    }
  }

  /**
   * Set whether a compact dimensional representation should be used,
   * i.e. whether singleton dimensions should be omitted.
   * Defaults to false.
   *
   * @param compact true for a compact dimensional representation
   */
  @Option(
      names = "--compact",
      description = "Only write dimensions greater than 1",
      defaultValue = "false"
  )
  public void setCompactDimensions(boolean compact) {
    compactDims = compact;
  }

  /**
   * Set the output dimension order. Defaults to XYZCT for compliance with
   * the OME-NGFF specification.
   *
   * @param order dimension order
   */
  @Deprecated
  @Option(
          names = "--dimension-order",
          description = "Override the input file dimension order in the " +
                  "output file [DEPRECRATED, results in " +
                  "invalid OME-NGFF data] " +
                  "(${COMPLETION-CANDIDATES})",
          converter = DimensionOrderConverter.class,
          defaultValue = "XYZCT"
  )
  public void setDimensionOrder(DimensionOrder order) {
    if (order != DimensionOrder.XYZCT) {
      LOGGER.warn(
        "--dimension-order is deprecated;" +
        " OME-NGFF requires XYZCT output order");
    }
    dimensionOrder = order;
  }

  // Option getters

  /**
   * @return path to input data
   */
  public String getInputPath() {
    return inputPath == null ? null : inputPath.toString();
  }

  /**
   * @return path to output data
   */
  public String getOutputPath() {
    return outputLocation;
  }

  /**
   * @return current output filesystem options
   */
  public Map<String, String> getOutputOptions() {
    return outputOptions;
  }

  /**
   * @return number of pyramid resolutions
   */
  public Integer getResolutions() {
    return pyramidResolutions;
  }

  /**
   * @return list of series indexes to convert
   */
  public List<Integer> getSeriesList() {
    return seriesList;
  }

  /**
   * @return tile width (X chunk size)
   */
  public int getTileWidth() {
    return tileWidth;
  }

  /**
   * @return tile height (Y chunk size)
   */
  public int getTileHeight() {
    return tileHeight;
  }

  /**
   * @return tile depth (Z chunk size)
   */
  public int getChunkDepth() {
    return chunkDepth;
  }

  /**
   * @return shard width (X shard size)
   */
  public int getShardWidth() {
    return shardWidth;
  }

  /**
   * @return shard height (Y shard size)
   */
  public int getShardHeight() {
    return shardHeight;
  }

  /**
   * @return shard depth (Z shard size)
   */
  public int getShardDepth() {
    return shardDepth;
  }

  /**
   * @return true if image data will not be converted
   */
  public boolean getNoTiles() {
    return !writeImageData;
  }

  /**
   * @return slf4j logging level
   */
  public String getLogLevel() {
    return logLevel;
  }

  /**
   * @return true if progress bars are displayed
   */
  public boolean getProgressBars() {
    return progressBars;
  }

  /**
   * @return true if a warning is logged instead of an exception when the tmp
   *              directory is noexec
   */
  public boolean getWarnNoExec() {
    return warnNoExec;
  }

  /**
   * @return true if only version info is displayed
   */
  public boolean getPrintVersionOnly() {
    return printVersion;
  }

  /**
   * @return true if only usage info is displayed
   */
  public boolean getHelp() {
    return help;
  }

  /**
   * @return maximum number of worker threads
   */
  public int getMaxWorkers() {
    return maxWorkers;
  }

  /**
   * @return maximum number of cached input tiles
   */
  public int getMaxCachedTiles() {
    return maxCachedTiles;
  }

  /**
   * @return current compression type
   */
  public ZarrCompression getCompression() {
    return compressionType;
  }

  /**
   * @return compression options
   */
  public Map<String, Object> getCompressionProperties() {
    return compressionProperties;
  }

  /**
   * @return true if chunks within a shard are compressed
   */
  public boolean getCompressInnerChunk() {
    return compressInnerChunk;
  }

  /**
   * @return list of extra readers to use
   */
  public Class<?>[] getExtraReaders() {
    return extraReaders;
  }

  /**
   * @return true if min/max pixel values and OMERO rendering
   *         metadata are calculated
   */
  public boolean getCalculateOMEROMetadata() {
    return omeroMetadata;
  }

  /**
   * @return true if chunks are stored nested ('/' chunk separator)
   */
  public boolean getNested() {
    return nested;
  }

  /**
   * @return pyramid name
   */
  public String getPyramidName() {
    return pyramidName;
  }

  /**
   * @return scale formatting pattern
   */
  public String getScaleFormat() {
    return scaleFormatString;
  }

  /**
   * @return path to CSV file with additional scale formatting arguments
   */
  public Path getAdditionalScaleFormatCSV() {
    return additionalScaleFormatStringArgsCsv;
  }

  /**
   * @return true if Zarr v3 data should be written
   */
  public boolean getV3() {
    return v3;
  }

  /**
   * @return directory containing memo (metadata cache) files
   */
  public File getMemoDirectory() {
    return memoDirectory;
  }

  /**
   * @return true if memo (metadata cache) files are retained after conversion
   */
  public boolean getKeepMemoFiles() {
    return keepMemoFiles;
  }

  /**
   * @return downsampling algorithm
   */
  public Downsampling getDownsampling() {
    return downsampling;
  }

  /**
   * @return true if existing Zarr data can be overwritten
   */
  public boolean getOverwrite() {
    return overwrite;
  }

  /**
   * @return fill value for missing tiles (.mrxs input only)
   */
  public Short getFillValue() {
    return fillValue;
  }

  /**
   * See:
   * https://docs.openmicroscopy.org/bio-formats/latest/formats/options.html.
   *
   * @return reader-specific options
   */
  public List<String> getReaderOptions() {
    return readerOptions;
  }

  /**
   * See https://ngff.openmicroscopy.org/latest/#plate-md.
   *
   * @return false if HCS (plate/well) metadata is written when the input data
   *         represents a plate
   */
  public boolean getNoHCS() {
    return noHCS;
  }

  /**
   * See https://ngff.openmicroscopy.org/latest/#bf2raw.
   *
   * @return false if OME-XML metadata is written
   */
  public boolean getNoOMEMeta() {
    return noOMEMeta;
  }

  /**
   * @return true if original metadata will be written in the OME-XML
   */
  public boolean getOriginalMetadata() {
    return originalMetadata;
  }

  /**
   * @return true if a root Zarr group will not be written
   */
  public boolean getNoRootGroup() {
    return noRootGroup;
  }

  /**
   * @return true if resolutions in the input data are converted
   */
  public boolean getReuseExistingResolutions() {
    return reuseExistingResolutions;
  }

  /**
   * @return target size of the largest XY dimension in the smallest
   *         pyramid resolution
   */
  public int getMinImageSize() {
    return minSize;
  }

  /**
   * @return true if dimensions have been compacted
   */
  public boolean getCompactDimensions() {
    return compactDims;
  }

  /**
   * @return current dimension roder
   */
  public DimensionOrder getDimensionOrder() {
    return dimensionOrder;
  }

  // Conversion methods

  /**
   * @return 0 if conversion completed without error,
   *         -1 if conversion was not performed
   * @throws Exception on most conversion errors
   */
  @Override
  public Integer call() throws Exception {
    ch.qos.logback.classic.Logger root = (ch.qos.logback.classic.Logger)
        LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME);
    root.setLevel(Level.toLevel(logLevel));

    if (help) {
      return -1;
    }

    if (printVersion) {
      String version = Optional.ofNullable(
        this.getClass().getPackage().getImplementationVersion()
        ).orElse("development");
      System.out.println("Version = " + version);
      System.out.println("Bio-Formats version = " + FormatTools.VERSION);
      System.out.println("NGFF specification version = " + getNGFFVersion());
      return -1;
    }

    if (inputPath == null) {
      throw new IllegalArgumentException("Input path not specified");
    }
    if (outputLocation == null) {
      throw new IllegalArgumentException("Output location not specified");
    }

    if (fillValue != null && (fillValue < 0 || fillValue > 255)) {
      throw new IllegalArgumentException("Invalid fill value: " + fillValue);
    }
    if (tileWidth != 1024 || tileHeight != 1024) {
      LOGGER.warn("Non-default tile size: {} x {}. " +
        "This may cause performance issues in some applications.",
        tileWidth, tileHeight);
    }

    // if the tmpdir is mounted as noexec, then any native library
    // loading (OpenCV, turbojpeg, etc.) is expected to fail, which
    // can cause conversion to fail with an exception that is not user friendly
    //
    // try to detect this case early and fail before the conversion begins,
    // with a more informative message

    // a temp file is created and set as executable
    // in the noexec case, setting as executable is expected to silently fail
    File tmpdirCheck = File.createTempFile("noexec-test", ".txt");
    try {
      // expect 'success' to be true in the noexec case, even though
      // the file will not actually be executable
      boolean success = tmpdirCheck.setExecutable(true);
      if (!success || !tmpdirCheck.canExecute()) {
        String msg = System.getProperty("java.io.tmpdir") +
          " is noexec; fix it or specify a different java.io.tmpdir." +
          " See https://github.com/glencoesoftware/bioformats2raw/" +
          "blob/master/README.md#temporary-directory-usage for details";
        if (getWarnNoExec()) {
          LOGGER.warn(msg);
        }
        else {
          throw new RuntimeException(msg);
        }
      }
    }
    finally {
      tmpdirCheck.delete();
    }

    OpenCVTools.loadOpenCV();

    if (progressBars) {
      setProgressListener(new ProgressBarListener(logLevel));
    }

    if (outputLocation.contains("://")) {

      LOGGER.info("*** experimental remote filesystem support ***");

      URI uri = URI.create(outputLocation);
      URI endpoint = new URI(uri.getScheme(), uri.getUserInfo(), uri.getHost(),
              uri.getPort(), "", "", "");
      String path = uri.getRawPath().substring(1); // drop initial "/"
      int first = path.indexOf("/");
      String bucket = "/" + path.substring(0, first);
      String rest = path.substring(first + 1);

      LOGGER.debug("endpoint: {}", endpoint);
      LOGGER.debug("bucket: {}", bucket);
      LOGGER.debug("path: {}", rest);
      LOGGER.debug("opts: {}", outputOptions);

      FileSystem fs = FileSystems.newFileSystem(endpoint, outputOptions);
      outputPath = fs.getPath(bucket, rest);
      if (Files.exists(outputPath)) {
        if (overwrite) {
          LOGGER.warn("overwriting on remote filesystem not yet supported");
        }
        throw new IllegalArgumentException(
                "Output path " + outputPath + " already exists.");
      }
    }
    else {
      outputPath = Paths.get(outputLocation);

      if (Files.exists(outputPath)) {
        if (!overwrite) {
          throw new IllegalArgumentException(
                  "Output path " + outputPath + " already exists");
        }
        LOGGER.warn("Overwriting output path {}", outputPath);
        Files.walk(outputPath)
                .sorted(Comparator.reverseOrder())
                .map(Path::toFile)
                .forEach(File::delete);
      }
    }

    readers = new ArrayBlockingQueue<IFormatReader>(maxWorkers);
    queue = new LimitedQueue<Runnable>(maxWorkers);
    executor = new ThreadPoolExecutor(
      maxWorkers, maxWorkers, 0L, TimeUnit.MILLISECONDS, queue);
    convert();
    return 0;
  }

  /**
   * Perform the pyramid conversion according to the specified
   * command line arguments.
   *
   * @throws FormatException
   * @throws IOException
   * @throws InterruptedException
   * @throws EnumerationException
   */
  public void convert()
      throws FormatException, IOException, InterruptedException,
             EnumerationException, ZarrException
  {
    checkOutputPaths();

    Cache<TilePointer, byte[]> tileCache = CacheBuilder.newBuilder()
        .maximumSize(maxCachedTiles)
        .build();

    // First find which reader class we need
    Class<?> readerClass = getBaseReaderClass();

    // Now with our found type instantiate our queue of readers for use
    // during conversion
    boolean savedMemoFile = false;
    for (int i=0; i < maxWorkers; i++) {
      IFormatReader reader;
      Memoizer memoizer;
      try {
        reader = (IFormatReader) readerClass.getConstructor().newInstance();
        if (fillValue != null) {
          reader.setFillColor(fillValue.byteValue());
        }
        memoizer = createMemoizer(reader);
      }
      catch (Exception e) {
        LOGGER.error("Failed to instantiate reader: {}", readerClass, e);
        return;
      }

      if (readerOptions.size() > 0) {
        DynamicMetadataOptions options = new DynamicMetadataOptions();
        for (String option : readerOptions) {
          String[] pair = option.split("=");
          if (pair.length == 2) {
            options.set(pair[0], pair[1]);
          }
        }
        memoizer.setMetadataOptions(options);
      }

      memoizer.setOriginalMetadataPopulated(!noOMEMeta && originalMetadata);
      memoizer.setFlattenedResolutions(false);
      memoizer.setMetadataFiltered(true);
      memoizer.setMetadataStore(createMetadata());
      ChannelSeparator separator = new ChannelSeparator(memoizer);
      separator.setId(inputPath.toString());
      separator.setResolution(0);
      if (reader instanceof MiraxReader) {
        ((MiraxReader) reader).setTileCache(tileCache);
      }
      if (omeroMetadata) {
        readers.add(new MinMaxCalculator(separator));
      }
      else {
        readers.add(separator);
      }
      savedMemoFile = savedMemoFile || memoizer.isSavedToMemo();
    }

    // Finally, perform conversion on all series
    try {
      IFormatReader v = readers.take();
      IMetadata meta = null;
      try {
        meta = (IMetadata) v.getMetadataStore();
        ((OMEXMLMetadata) meta).resolveReferences();

        if (!noHCS) {
          noHCS = !hasValidPlate(meta);
          int plateCount = meta.getPlateCount();
          if (!noHCS && plateCount > 1) {
            throw new IllegalArgumentException(
              "Found " + plateCount + " plates; only one can be converted. " +
              "Use --no-hcs to as a work-around.");
          }
        }
        else {
          ((OMEXMLMetadata) meta).resolveReferences();
          OMEXMLMetadataRoot root = (OMEXMLMetadataRoot) meta.getRoot();
          for (int i=0; i<meta.getPlateCount(); i++) {
            root.removePlate(root.getPlate(0));
          }
          meta.setRoot(root);
        }

        if (seriesList.size() > 0) {
          ((OMEXMLMetadata) meta).resolveReferences();
          OMEXMLMetadataRoot root = (OMEXMLMetadataRoot) meta.getRoot();
          int toRemove = meta.getImageCount();

          for (Integer index : seriesList) {
            if (index >= 0 && index < toRemove) {
              root.addImage(root.getImage(index));
            }
          }

          for (int i=0; i<toRemove; i++) {
            root.removeImage(root.getImage(0));
          }
          meta.setRoot(root);
        }
        else {
          for (int i=0; i<meta.getImageCount(); i++) {
            seriesList.add(i);
          }
        }

        for (int s=0; s<meta.getImageCount(); s++) {
          meta.setPixelsBigEndian(true, s);

          if (dimensionOrder != null) {
            meta.setPixelsDimensionOrder(dimensionOrder, s);
          }

          if (!noHCS) {
            HCSIndex index = new HCSIndex(meta, s);
            hcsIndexes.add(index);
          }
        }

        if (!noOMEMeta) {
          OMEXMLService service = getService();
          service.removeBinData((OMEXMLMetadata) meta);
          service.removeTiffData((OMEXMLMetadata) meta);
          for (int s=0; s<meta.getImageCount(); s++) {
            service.addMetadataOnly((OMEXMLMetadata) meta, s, s == 0);
          }
          String xml = service.getOMEXML(meta);

          // write the original OME-XML to a file
          Path metadataPath = getRootPath().resolve("OME");
          if (!Files.exists(metadataPath)) {
            Files.createDirectories(metadataPath);
          }
          Path omexmlFile = metadataPath.resolve(METADATA_FILE);
          Files.write(omexmlFile, xml.getBytes(Constants.ENCODING));
        }
      }
      catch (ServiceException se) {
        LOGGER.error("Could not retrieve OME-XML", se);
        return;
      }
      finally {
        readers.put(v);
      }
      if (meta != null) {
        final OMEXMLMetadataRoot root = (OMEXMLMetadataRoot) meta.getRoot();
        readers.forEach((reader) -> {
          IMetadata workerMeta = (IMetadata) reader.getMetadataStore();
          workerMeta.setRoot(root);
        });
      }

      if (!noHCS) {
        scaleFormatString = "%s/%s/%d/%d";
      }

      writeZarrMetadata();

      // pre-calculate resolution and tile counts
      long totalTiles = 0;
      for (Integer index : seriesList) {
        int[] counts = calculateTileCounts(index);
        tileCounts.put(index, counts);
        for (int count : counts) {
          totalTiles += count;
        }
      }
      getProgressListener().notifyStart(seriesList.size(), totalTiles);

      for (Integer index : seriesList) {
        try {
          write(index);
        }
        catch (Throwable t) {
          LOGGER.error("Error while writing series {}", index, t);
          unwrapException(t);
          return;
        }
      }

      if (meta != null) {
        saveHCSMetadata(meta);
      }
    }
    finally {
      // Shut down first, tasks may still be running
      executor.shutdown();
      executor.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);
      readers.forEach((v) -> {
        try {
          v.close();
        }
        catch (IOException e) {
          LOGGER.error("Exception while closing reader", e);
        }
      });
    }

    // delete the memo file if it was saved and it's not explicitly kept
    // this should mean that memo files which existed before conversion
    // started will not be deleted
    if (savedMemoFile && !keepMemoFiles) {
      File memoFile = createMemoizer(null).getMemoFile(inputPath.toString());
      memoFile.delete();
    }
  }

  private void writeZarrMetadata() throws IOException, ZarrException {
    if (getV3() && v3Store == null) {
      v3Store = new FilesystemStore(getRootPath());
    }

    // fileset level metadata
    if (!noRootGroup) {
      Map<String, Object> attributes = new HashMap<String, Object>();
      attributes.put("bioformats2raw.layout", LAYOUT);

      if (getV3()) {
        Group v3Root = Group.create(v3Store.resolve());
        v3Root.setAttributes(attributes);
      }
      else {
        final ZarrGroup root = ZarrGroup.create(getRootPath());
        root.writeAttributes(attributes);
      }
    }
    if (!noOMEMeta) {
      Map<String, Object> attributes = new HashMap<String, Object>();

      // record the path to each series (multiscales) and the corresponding
      // series (OME-XML Image) index
      // using the index as the key would mean that the index is stored
      // as a string instead of an integer
      List<String> groups = new ArrayList<String>();
      for (Integer index : seriesList) {
        String resolutionString = String.format(
                scaleFormatString, getScaleFormatStringArgs(index, 0));
        String seriesString = "";
        if (resolutionString.indexOf('/') >= 0) {
          seriesString = resolutionString.substring(0,
              resolutionString.lastIndexOf('/'));
        }
        groups.add(seriesString);
      }
      attributes.put("series", groups);

      if (getV3()) {
        Group v3OME = Group.create(v3Store.resolve("OME"));
        v3OME.setAttributes(attributes);
      }
      else {
        Path metadataPath = getRootPath().resolve("OME");
        final ZarrGroup root = ZarrGroup.create(metadataPath);
        root.writeAttributes(attributes);
      }
    }
  }

  /**
   * Pre-calculate the number of resolutions and tiles for the
   * given series index. This is useful for accurate progress reporting
   * and failing faster for incompatible pixel/downsampling combinations.
   *
   * @param series series index
   * @return array of tile counts; the array length is the number of resolutions
   */
  private int[] calculateTileCounts(int series)
    throws FormatException, IOException, InterruptedException,
           EnumerationException
  {
    readers.forEach((reader) -> {
      reader.setSeries(series);
    });

    IFormatReader workingReader = readers.take();
    int resolutions = 1;
    int sizeX;
    int sizeY;
    int sizeZ;
    int imageCount;
    try {
      // calculate a reasonable pyramid depth if not specified as an argument
      sizeX = workingReader.getSizeX();
      sizeY = workingReader.getSizeY();
      if (pyramidResolutions == null) {
        if (workingReader.getResolutionCount() > 1
            && reuseExistingResolutions)
        {
          resolutions = workingReader.getResolutionCount();
        }
        else {
          resolutions = calculateResolutions(sizeX, sizeY);
        }
      }
      else {
        resolutions = pyramidResolutions;

        // check to make sure too many resolutions aren't being used
        if ((int) (sizeX / Math.pow(PYRAMID_SCALE, resolutions)) == 0 ||
          (int) (sizeY / Math.pow(PYRAMID_SCALE, resolutions)) == 0)
        {
          resolutions = calculateResolutions(sizeX, sizeY);
          LOGGER.warn("Too many resolutions specified; reducing to {}",
            resolutions);
        }
      }
      LOGGER.info("Using {} pyramid resolutions", resolutions);
      sizeZ = workingReader.getSizeZ();
      imageCount = workingReader.getImageCount();
      pixelType = workingReader.getPixelType();
    }
    finally {
      readers.put(workingReader);
    }

    int[] resTileCounts = new int[resolutions];

    if ((pixelType == FormatTools.INT8 || pixelType == FormatTools.INT32) &&
      getDownsampling() != Downsampling.SIMPLE && resolutions > 0)
    {
      String type = FormatTools.getPixelTypeString(pixelType);
      throw new UnsupportedOperationException(
        "OpenCV does not support downsampling " + type + " data. " +
        "See https://github.com/opencv/opencv/issues/7862");
    }

    for (int resCounter=0; resCounter<resolutions; resCounter++) {
      final int resolution = resCounter;
      int scale = (int) Math.pow(PYRAMID_SCALE, resolution);
      int scaledWidth = sizeX / scale;
      int scaledHeight = sizeY / scale;
      int scaledDepth = sizeZ;

      workingReader = readers.take();
      try {
        if (workingReader.getResolutionCount() > 1
            && reuseExistingResolutions)
        {
          workingReader.setResolution(resCounter);
          scaledWidth = workingReader.getSizeX();
          scaledHeight = workingReader.getSizeY();
          scaledDepth = workingReader.getSizeZ();
        }
      }
      finally {
        readers.put(workingReader);
      }

      resTileCounts[resCounter] =
          (int) Math.ceil((double) scaledWidth / tileWidth)
          * (int) Math.ceil((double) scaledHeight / tileHeight)
          * (int) Math.ceil((double) scaledDepth / chunkDepth)
          * (imageCount / sizeZ);
    }
    return resTileCounts;
  }

  /**
   * Convert the data specified by the given initialized reader to
   * an intermediate form.
   *
   * @param series the reader series index to be converted
   * @throws FormatException
   * @throws IOException
   * @throws InterruptedException
   * @throws EnumerationException
   */
  public void write(int series)
    throws FormatException, IOException, InterruptedException,
           EnumerationException, ZarrException
  {
    readers.forEach((reader) -> {
      reader.setSeries(series);
    });
    saveResolutions(series);
  }

  /**
   * Create a new Memoizer that wraps the given reader, based on the
   * user-specified caching options. The supplied reader may be null.
   *
   * @param reader existing reader to wrap, or null
   * @return new uninitialized Memoizer instance
   */
  private Memoizer createMemoizer(IFormatReader reader) {
    if (memoDirectory == null) {
      if (reader == null) {
        return new Memoizer();
      }
      return new Memoizer(reader);
    }
    else {
      if (reader == null) {
        return new Memoizer(
          Memoizer.DEFAULT_MINIMUM_ELAPSED, memoDirectory);
      }
      else {
        return new Memoizer(
          reader, Memoizer.DEFAULT_MINIMUM_ELAPSED, memoDirectory);
      }
    }
  }

  /**
   * Get the root path of the dataset, which will contain Zarr and OME-XML.
   * By default, this is the specified output directory.
   * If "--pyramid-name" was used, then this will be the pyramid name
   * as a subdirectory of the output directory.
   *
   * @return directory into which Zarr and OME-XML data is written
   */
  private Path getRootPath() {
    if (pyramidName == null) {
      return outputPath;
    }
    return outputPath.resolve(pyramidName);
  }

  /**
   * Retrieves the scaled format string arguments, adding additional ones
   * from a provided CSV file.
   * @param series current series to be prepended to the argument list
   * @param resolution current resolution to be prepended to the argument list
   * @return Array of arguments to be used in format string application.  Order
   * will be <code>[series, resolution, &lt;additionalArgs&gt;...]</code> where
   * <code>additionalArgs</code> is sourced from an optionally provided CSV
   * file.
   */
  private Object[] getScaleFormatStringArgs(
      Integer series, Integer resolution)
  {
    List<Object> args = new ArrayList<Object>();
    if (!noHCS) {
      HCSIndex index = hcsIndexes.get(series);
      args.add(index.getRowPath());
      args.add(index.getColumnPath());
      args.add(index.getFieldIndex());
      args.add(resolution);
    }
    else {
      // if a single series is written,
      // the output series index should always be 0
      args.add(seriesList.indexOf(series));
      args.add(resolution);
      if (additionalScaleFormatStringArgs != null) {
        String[] row = additionalScaleFormatStringArgs.get(series);
        for (String arg : row) {
          args.add(arg);
        }
      }
    }
    return args.toArray();
  }

  /**
   * Read tile as bytes from typed Zarr v3 array.
   * @param array Zarr array to read from
   * @param shape array describing the number of elements in each dimension to
   * be read
   * @param offset array describing the offset in each dimension at which to
   * begin reading
   * @return tile data as bytes of size <code>shape * bytesPerPixel</code>
   * read from <code>offset</code>.
   */
  public static byte[] readAsBytesV3(Array array, int[] shape, int[] offset)
    throws ZarrException
  {
    ucar.ma2.Array tile = array.read(Utils.toLongArray(offset), shape);
    ByteBuffer buf = tile.getDataAsByteBuffer(ByteOrder.BIG_ENDIAN);
    byte[] bytes = new byte[buf.remaining()];
    buf.get(bytes);
    return bytes;
  }

  /**
   * Read tile as bytes from typed Zarr array.
   * @param zArray Zarr array to read from
   * @param shape array describing the number of elements in each dimension to
   * be read
   * @param offset array describing the offset in each dimension at which to
   * begin reading
   * @return tile data as bytes of size <code>shape * bytesPerPixel</code>
   * read from <code>offset</code>.
   * @throws IOException
   * @throws InvalidRangeException
   */
  public static byte[] readAsBytes(ZarrArray zArray, int[] shape, int[] offset)
      throws IOException, InvalidRangeException
  {
    DataType dataType = zArray.getDataType();
    int bytesPerPixel = ZarrTypes.bytesPerPixel(dataType);
    int size = IntStream.of(shape).reduce((a, b) -> a * b).orElse(0);
    byte[] tileAsBytes = new byte[size * bytesPerPixel];
    ByteBuffer tileAsByteBuffer = ByteBuffer.wrap(tileAsBytes);
    switch (dataType) {
      case i1:
      case u1: {
        zArray.read(tileAsBytes, shape, offset);
        break;
      }
      case i2:
      case u2: {
        short[] tileAsShorts = new short[size];
        zArray.read(tileAsShorts, shape, offset);
        tileAsByteBuffer.asShortBuffer().put(tileAsShorts);
        break;
      }
      case i4:
      case u4: {
        int[] tileAsInts = new int[size];
        zArray.read(tileAsInts, shape, offset);
        tileAsByteBuffer.asIntBuffer().put(tileAsInts);
        break;
      }
      case f4: {
        float[] tileAsFloats = new float[size];
        zArray.read(tileAsFloats, shape, offset);
        tileAsByteBuffer.asFloatBuffer().put(tileAsFloats);
        break;
      }
      case f8: {
        double[] tileAsDoubles = new double[size];
        zArray.read(tileAsDoubles, shape, offset);
        tileAsByteBuffer.asDoubleBuffer().put(tileAsDoubles);
        break;
      }
      default:
        throw new IllegalArgumentException(
            "Unsupported data type: " + dataType);
    }
    return tileAsBytes;
  }

  /**
   * Write tile as bytes to typed Zarr array.
   * @param pathName path to Zarr array
   * @param shape array describing the number of elements in each dimension to
   * be written
   * @param offset array describing the offset in each dimension at which to
   * begin writing
   * @param tile data as bytes of size <code>shape * bytesPerPixel</code> to be
   * written at <code>offset</code>
   * @throws IOException
   * @throws InvalidRangeException
   */
  private void writeBytes(
      String pathName, int[] shape, int[] offset, ByteBuffer tile)
          throws IOException, InvalidRangeException, ZarrException
  {
    if (getV3()) {
      writeBytesV3(Array.open(v3Store.resolve(pathName)), shape, offset, tile);
    }
    else {
      writeBytesV2(ZarrArray.open(getRootPath().resolve(pathName)),
        shape, offset, tile);
    }
  }

  private static void writeBytesV3(
      Array array, int[] shape, int[] offset, ByteBuffer tile)
        throws ZarrException
  {
    final ucar.ma2.Array pixels = ucar.ma2.Array.factory(
      array.metadata.dataType.getMA2DataType(), shape, tile);
    array.write(Utils.toLongArray(offset), pixels);
  }

  private static void writeBytesV2(
      ZarrArray zArray, int[] shape, int[] offset, ByteBuffer tile)
        throws IOException, InvalidRangeException
  {
    int size = IntStream.of(shape).reduce((a, b) -> a * b).orElse(0);
    DataType dataType = zArray.getDataType();
    Slf4JStopWatch t1 = stopWatch();
    try {
      switch (dataType) {
        case i1:
        case u1: {
          zArray.write(tile.array(), shape, offset);
          break;
        }
        case i2:
        case u2: {
          short[] tileAsShorts = new short[size];
          tile.asShortBuffer().get(tileAsShorts);
          zArray.write(tileAsShorts, shape, offset);
          break;
        }
        case i4:
        case u4: {
          int[] tileAsInts = new int[size];
          tile.asIntBuffer().get(tileAsInts);
          zArray.write(tileAsInts, shape, offset);
          break;
        }
        case f4: {
          float[] tileAsFloats = new float[size];
          tile.asFloatBuffer().get(tileAsFloats);
          zArray.write(tileAsFloats, shape, offset);
          break;
        }
        case f8: {
          double[] tileAsDoubles = new double[size];
          tile.asDoubleBuffer().put(tileAsDoubles);
          zArray.write(tileAsDoubles, shape, offset);
          break;
        }
        default:
          throw new IllegalArgumentException(
              "Unsupported data type: " + dataType);
      }
    }
    finally {
      t1.stop("writeBytes");
    }
  }

  private byte[] getTileDownsampled(
      int series, int resolution, int plane,
      Region boundingBox, List<Axis> axes)
          throws FormatException, IOException, InterruptedException,
                 EnumerationException, InvalidRangeException,
                 ZarrException
  {
    final String pathName =
        String.format(scaleFormatString,
            getScaleFormatStringArgs(series, resolution - 1));

    // Upscale our base X and Y offsets to the previous resolution
    // based on the pyramid scaling factor
    int xx = boundingBox.x * PYRAMID_SCALE;
    int yy = boundingBox.y * PYRAMID_SCALE;
    IFormatReader reader = readers.take();
    int[] offset;
    try {
      offset = getOffset(reader, xx, yy, plane, axes);
    }
    finally {
      readers.put(reader);
    }

    int[] dimensions = null;
    int[] blockSizes = null;

    ZarrArray v2Array = null;
    Array v3Array = null;

    if (getV3()) {
      v3Array = Array.open(v3Store.resolve(pathName));
      dimensions = Utils.toIntArray(v3Array.metadata.shape);
      blockSizes = v3Array.metadata.chunkShape();

      Optional<Codec> shardingCodec =
        ArrayMetadata.getShardingIndexedCodec(v3Array.metadata.codecs);
      if (shardingCodec.isPresent() &&
        shardingCodec.get() instanceof ShardingIndexedCodec)
      {
        ShardingIndexedCodec shardIndex =
          (ShardingIndexedCodec) shardingCodec.get();
        blockSizes = shardIndex.configuration.chunkShape;
      }
    }
    else {
      v2Array = ZarrArray.open(getRootPath().resolve(pathName));
      dimensions = v2Array.getShape();
      blockSizes = v2Array.getChunks();
    }

    int xDim = 1;
    int yDim = 1;
    int activeTileWidth = 1;
    int activeTileHeight = 1;
    for (int i=0; i<axes.size(); i++) {
      switch (axes.get(i).getType()) {
        case 'X':
          activeTileWidth = blockSizes[i];
          xDim = dimensions[i];
          break;
        case 'Y':
          activeTileHeight = blockSizes[i];
          yDim = dimensions[i];
          break;
        default:
          LOGGER.trace("ignoring axis type {}", axes.get(i).getType());
      }
    }

    int width = (int) Math.min(activeTileWidth * PYRAMID_SCALE, xDim - xx);
    int height = (int) Math.min(activeTileHeight * PYRAMID_SCALE, yDim - yy);

    int[] shape = createShape(axes, width, height, 1);
    byte[] tileAsBytes = null;
    if (getV3()) {
      tileAsBytes = readAsBytesV3(v3Array, shape, offset);
    }
    else {
      tileAsBytes = readAsBytes(v2Array, shape, offset);
    }
    int bytesPerPixel = FormatTools.getBytesPerPixel(pixelType);

    if (downsampling == Downsampling.SIMPLE) {
      return scaler.downsample(tileAsBytes, width, height,
        PYRAMID_SCALE, bytesPerPixel, false,
        FormatTools.isFloatingPoint(pixelType),
        1, false);
    }

    return OpenCVTools.downsample(
      tileAsBytes, pixelType, width, height, PYRAMID_SCALE, downsampling);
  }

  private byte[] getTile(
      int series, int resolution, int plane, Region boundingBox,
      List<Axis> axes)
          throws FormatException, IOException, InterruptedException,
                 EnumerationException, InvalidRangeException,
                 ZarrException
  {
    IFormatReader reader = readers.take();
    try {
      if (reader.getResolutionCount() > 1 && reuseExistingResolutions) {
        reader.setResolution(resolution);
        return reader.openBytes(plane,
          boundingBox.x, boundingBox.y,
          boundingBox.width, boundingBox.height);
      }
    }
    finally {
      readers.put(reader);
    }
    if (resolution == 0) {
      reader = readers.take();
      try {
        return reader.openBytes(plane,
          boundingBox.x, boundingBox.y,
          boundingBox.width, boundingBox.height);
      }
      finally {
        readers.put(reader);
      }
    }
    else {
      Slf4JStopWatch t0 = new Slf4JStopWatch("getTileDownsampled");
      try {
        return getTileDownsampled(series, resolution, plane, boundingBox, axes);
      }
      finally {
        t0.stop();
      }
    }
  }

  /**
   * Retrieve the dimensions based on either the configured or input file
   * dimension order at the current resolution.
   * @param reader initialized reader for the input file
   * @param scaledWidth size of the X dimension at the current resolution
   * @param scaledHeight size of the Y dimension at the current resolution
   * @param scaledDepth size of the Z dimension at the current resolution
   * @param scaledTileWidth chunk size in X
   * @param scaledTileHeight chunk size in Y
   * @param scaledChunkDepth chunk size in Z
   * @return dimension array ready for use with Zarr
   * @throws EnumerationException
   */
  private List<Axis> getDimensions(
    IFormatReader reader, int scaledWidth, int scaledHeight, int scaledDepth,
      int scaledTileWidth, int scaledTileHeight, int scaledChunkDepth)
      throws EnumerationException
  {
    ArrayList<Axis> axes = new ArrayList<Axis>();
    int sizeZ = reader.getSizeZ();
    int sizeC = reader.getSizeC();
    int sizeT = reader.getSizeT();
    String o = new StringBuilder(
        dimensionOrder != null? dimensionOrder.toString()
        : reader.getDimensionOrder()).reverse().toString();

    int spatialDims = 0;
    for (char c : o.toCharArray()) {
      switch (c) {
        case 'X':
          axes.add(new Axis(c, scaledWidth, scaledTileWidth));
          spatialDims++;
          break;
        case 'Y':
          axes.add(new Axis(c, scaledHeight, scaledTileHeight));
          spatialDims++;
          break;
        case 'Z':
          axes.add(new Axis(c, scaledDepth, scaledChunkDepth));
          spatialDims++;
          break;
        case 'C':
          axes.add(new Axis(c, sizeC, 1));
          break;
        case 'T':
          axes.add(new Axis(c, sizeT, 1));
          break;
        default:
          LOGGER.trace("ignoring axis type {}", c);
      }
    }

    if (getCompactDimensions()) {
      // if requested, omit any dimension that has length 1
      // this includes X and Y
      for (int a=0; a<axes.size(); a++) {
        Axis axis = axes.get(a);
        if (axis.getLength() == 1) {
          char type = axis.getType();
          if (type == 'X' || type == 'Y' || type == 'Z') {
            spatialDims--;
          }
          axes.remove(axis);
          a--;
        }
      }
    }

    if (spatialDims < 2) {
      throw new IllegalArgumentException("Found " + spatialDims +
        " spatial dimensions, try again without --compact");
    }
    return axes;
  }

  private int[] getShapeArray(List<Axis> axes) {
    int[] shape = new int[axes.size()];
    for (int i=0; i<axes.size(); i++) {
      shape[i] = axes.get(i).getLength();
    }
    return shape;
  }

  private int[] getChunkSizeArray(List<Axis> axes) {
    int[] chunk = new int[axes.size()];
    for (int i=0; i<axes.size(); i++) {
      chunk[i] = axes.get(i).getChunkSize();
    }
    return chunk;
  }

  private int[] getShardSizeArray(List<Axis> axes) {
    int[] shard = new int[axes.size()];
    for (int i=0; i<axes.size(); i++) {
      Axis axis = axes.get(i);
      switch (axis.getType()) {
        case 'X':
          shard[i] = getShardWidth();
          break;
        case 'Y':
          shard[i] = getShardHeight();
          break;
        case 'Z':
          shard[i] = getShardDepth();
          break;
        default:
          shard[i] = axis.getChunkSize();
      }
    }
    return shard;
  }

  private int[] createShape(List<Axis> axes, int width, int height, int depth) {
    int[] shape = new int[axes.size()];
    Arrays.fill(shape, 1);
    for (int i=0; i<axes.size(); i++) {
      switch (axes.get(i).getType()) {
        case 'X':
          shape[i] = width;
          break;
        case 'Y':
          shape[i] = height;
          break;
        case 'Z':
          shape[i] = depth;
          break;
        default:
          LOGGER.trace("ignoring axis type {}", axes.get(i).getType());
      }
    }
    return shape;
  }

  /**
   * Retrieve the offset based on either the configured or input file
   * dimension order at the current resolution.
   * @param reader initialized reader for the input file
   * @param x X position at the current resolution
   * @param y Y position at the current resolution
   * @param plane current plane being operated upon
   * @param axes ordered list of typed axes (may have fewer than 5 elements)
   * @return offsets array ready to use
   * @throws EnumerationException
   */
  private int[] getOffset(
    IFormatReader reader, int x, int y, int plane, List<Axis> axes)
    throws EnumerationException
  {
    int[] zct = reader.getZCTCoords(plane);
    int[] offset = new int[axes.size()];
    Arrays.fill(offset, 0);
    for (int i=0; i<axes.size(); i++) {
      Axis a = axes.get(i);
      switch (a.getType()) {
        case 'X':
          offset[i] = x;
          break;
        case 'Y':
          offset[i] = y;
          break;
        case 'Z':
          offset[i] = zct[0];
          break;
        case 'C':
          offset[i] = zct[1];
          break;
        case 'T':
          offset[i] = zct[2];
          break;
        default:
          LOGGER.trace("ignoring axis type {}", axes.get(i).getType());
      }
    }
    return offset;
  }

  private void processChunk(int series, int resolution, int plane,
      int[] offset, int[] shape, List<Axis> axes)
        throws EnumerationException, FormatException, IOException,
          InterruptedException, InvalidRangeException, ZarrException
  {
    String pathName =
        String.format(scaleFormatString,
            getScaleFormatStringArgs(series, resolution));
    IFormatReader reader = readers.take();
    boolean littleEndian = reader.isLittleEndian();
    int bpp = FormatTools.getBytesPerPixel(reader.getPixelType());
    int[] zct;
    ByteArrayOutputStream chunkAsBytes = new ByteArrayOutputStream();
    int xOffset = 0;
    int xShape = 1;
    int yOffset = 0;
    int yShape = 1;
    int zOffset = 0;
    int zShape = 1;
    try {
      LOGGER.info("requesting tile to write at {} to {}", offset, pathName);
      //Get coords of current series
      zct = reader.getZCTCoords(plane);
      for (int i=0; i<axes.size(); i++) {
        switch (axes.get(i).getType()) {
          case 'X':
            xOffset = offset[i];
            xShape = shape[i];
            break;
          case 'Y':
            yOffset = offset[i];
            yShape = shape[i];
            break;
          case 'Z':
            zOffset = offset[i];
            zShape = shape[i];
            break;
          default:
            LOGGER.trace("ignoring axis type {}", axes.get(i).getType());
        }
      }
    }
    finally {
      readers.put(reader);
    }
    getProgressListener().notifyChunkStart(
      plane, xOffset, yOffset, zOffset);
    Slf4JStopWatch t0 = new Slf4JStopWatch("getChunk");
    try {
      for (int z = zOffset; z < zOffset + zShape; z++) {
        //Get plane index for current Z
        reader = readers.take();
        int planeIndex;
        try {
          planeIndex = FormatTools.getIndex(reader, z, zct[1], zct[2]);
        }
        finally {
          readers.put(reader);
        }
        Region boundingBox = new Region(xOffset, yOffset, xShape, yShape);
        byte[] tileAsBytes = getTile(series, resolution, planeIndex,
                                    boundingBox, axes);
        if (tileAsBytes == null) {
          return;
        }
        chunkAsBytes.write(tileAsBytes);
      }
    }
    finally {
      nTile.incrementAndGet();
      LOGGER.info("chunk read complete {}/{}", nTile.get(), tileCount);
      t0.stop();
    }
    ByteBuffer tileBuffer = ByteBuffer.wrap(chunkAsBytes.toByteArray());
    if (resolution == 0 && bpp > 1) {
      tileBuffer.order(
        littleEndian ? ByteOrder.LITTLE_ENDIAN : ByteOrder.BIG_ENDIAN);
    }
    writeBytes(pathName, shape, offset, tileBuffer);
    getProgressListener().notifyChunkEnd(plane, xOffset, yOffset, zOffset);
  }

  /**
   * Write all resolutions for the current series to an intermediate form.
   * Readers should be initialized and have the correct series state.
   *
   * @param series the reader series index to be converted
   * @throws FormatException
   * @throws IOException
   * @throws InterruptedException
   * @throws EnumerationException
   */
  public void saveResolutions(int series)
    throws FormatException, IOException, InterruptedException,
           EnumerationException, ZarrException
  {
    int[] resTileCounts = tileCounts.get(series);
    int resolutions = resTileCounts.length;
    int seriesTiles = 0;
    for (int t : resTileCounts) {
      seriesTiles += t;
    }
    getProgressListener().notifySeriesStart(series, resolutions, seriesTiles);

    IFormatReader workingReader = readers.take();
    int sizeX;
    int sizeY;
    int sizeZ;
    int sizeT;
    int sizeC;
    int imageCount;
    String readerDimensionOrder;
    try {
      // calculate a reasonable pyramid depth if not specified as an argument
      sizeX = workingReader.getSizeX();
      sizeY = workingReader.getSizeY();
      sizeZ = workingReader.getSizeZ();
      sizeT = workingReader.getSizeT();
      sizeC = workingReader.getSizeC();
      readerDimensionOrder = workingReader.getDimensionOrder();
      imageCount = workingReader.getImageCount();
      pixelType = workingReader.getPixelType();
    }
    finally {
      readers.put(workingReader);
    }

    LOGGER.info(
      "Preparing to write pyramid sizeX {} (tileWidth: {}) " +
      "sizeY {} (tileWidth: {}) sizeZ {} (tileDepth: {}) imageCount {}",
        sizeX, tileWidth, sizeY, tileHeight, sizeZ, chunkDepth, imageCount
    );

    // series level metadata
    // do this after pixels are written, otherwise
    // min/max calculation won't have been performed
    setSeriesLevelMetadata(series, resolutions);

    for (int resCounter=0; resCounter<resolutions; resCounter++) {
      final int resolution = resCounter;
      int scale = (int) Math.pow(PYRAMID_SCALE, resolution);
      int scaledWidth = sizeX / scale;
      int scaledHeight = sizeY / scale;
      int scaledDepth = sizeZ;

      workingReader = readers.take();
      try {
        if (workingReader.getResolutionCount() > 1
            && reuseExistingResolutions)
        {
          workingReader.setResolution(resCounter);
          scaledWidth = workingReader.getSizeX();
          scaledHeight = workingReader.getSizeY();
          scaledDepth = workingReader.getSizeZ();
        }
      }
      finally {
        readers.put(workingReader);
      }

      int activeTileWidth = tileWidth;
      int activeTileHeight = tileHeight;
      int activeChunkDepth = chunkDepth;
      if (scaledWidth < activeTileWidth) {
        LOGGER.info("Reducing active tileWidth to {}", scaledWidth);
        activeTileWidth = scaledWidth;
      }

      if (scaledHeight < activeTileHeight) {
        LOGGER.info("Reducing active tileHeight to {}", scaledHeight);
        activeTileHeight = scaledHeight;
      }

      if (scaledDepth < activeChunkDepth) {
        LOGGER.warn("Reducing active chunkDepth to {}", scaledDepth);
        activeChunkDepth = scaledDepth;
      }

      List<Axis> activeAxes =
        getDimensions(workingReader, scaledWidth, scaledHeight, scaledDepth,
        activeTileWidth, activeTileHeight, activeChunkDepth);

      String resolutionString = String.format(
              scaleFormatString, getScaleFormatStringArgs(series, resolution));

      if (getV3()) {
        CodecBuilder codecBuilder =
          new CodecBuilder(ZarrTypes.getV3ZarrType(pixelType));

        int[] chunkSizes = getChunkSizeArray(activeAxes);
        int[] shardSizes = getShardSizeArray(activeAxes);
        int[] shape = getShapeArray(activeAxes);
        boolean useSharding = false;

        for (int axis=0; axis<shape.length; axis++) {
          // for smaller resolutions in particular, the selected
          // chunk size may be smaller than the image
          // the chunk size needs to be adjusted in this case,
          // otherwise an exception will be thrown when creating the array
          if (shape[axis] < shardSizes[axis]) {
            shardSizes[axis] = shape[axis];
          }
        }

        if (chunkAndShardCompatible(chunkSizes, shardSizes, shape)) {
          useSharding = true;
          if (getCompressInnerChunk()) {
            final CodecBuilder innerChunkBuilder =
              applyCompressionType(
                new CodecBuilder(ZarrTypes.getV3ZarrType(pixelType)));
            codecBuilder = codecBuilder.withSharding(chunkSizes,
              c -> innerChunkBuilder);
          }
          else {
            codecBuilder = codecBuilder.withSharding(chunkSizes);
          }
        }
        if (!getCompressInnerChunk()) {
          codecBuilder = applyCompressionType(codecBuilder);
        }

        String[] dimensionNames = new String[activeAxes.size()];
        for (int a=0; a<activeAxes.size(); a++) {
          dimensionNames[a] =
            String.valueOf(activeAxes.get(a).getType()).toLowerCase();
        }

        final CodecBuilder builder = codecBuilder;
        StoreHandle v3Handle = v3Store.resolve(resolutionString);
        Array v3Array = Array.create(v3Handle,
          Array.metadataBuilder()
            .withDimensionNames(dimensionNames)
            .withShape(Utils.toLongArray(shape))
            .withDataType(ZarrTypes.getV3ZarrType(pixelType))
            .withChunkShape(useSharding ? shardSizes : chunkSizes)
            .withCodecs(c -> builder)
            .build()
        );
      }
      else {
        DataType dataType = ZarrTypes.getZarrType(pixelType);
        ArrayParams arrayParams = new ArrayParams()
            .shape(getShapeArray(activeAxes))
            .chunks(getChunkSizeArray(activeAxes))
            .dataType(dataType)
            .dimensionSeparator(getDimensionSeparator())
            .compressor(CompressorFactory.create(
                compressionType.toString(), compressionProperties));
        ZarrArray.create(getRootPath().resolve(resolutionString), arrayParams);
      }

      if (!writeImageData) {
        continue;
      }

      nTile = new AtomicInteger(0);
      tileCount = resTileCounts[resolution];

      List<CompletableFuture<Void>> futures =
        new ArrayList<CompletableFuture<Void>>();

      getProgressListener().notifyResolutionStart(resolution, tileCount);

      try {
        for (int j=0; j<scaledHeight; j+=tileHeight) {
          final int yy = j;
          int height = (int) Math.min(tileHeight, scaledHeight - yy);
          for (int k=0; k<scaledWidth; k+=tileWidth) {
            final int xx = k;
            int width = (int) Math.min(tileWidth, scaledWidth - xx);
            for (int l=0; l<scaledDepth; l+=chunkDepth) {
              final int zz = l;
              int depth = (int) Math.min(chunkDepth, scaledDepth - zz);
              for (int cc=0; cc<sizeC; cc++) {
                for (int tt=0; tt<sizeT; tt++) {
                  final int plane = FormatTools.getIndex(readerDimensionOrder,
                      sizeZ, sizeC, sizeT, imageCount, zz, cc, tt);

                  CompletableFuture<Void> future =
                       new CompletableFuture<Void>();
                  futures.add(future);
                  executor.execute(() -> {
                    try {
                      int[] shape =
                        createShape(activeAxes, width, height, depth);
                      int[] offset;
                      IFormatReader reader = readers.take();
                      try {
                        offset = getOffset(reader, xx, yy, plane, activeAxes);
                      }
                      finally {
                        readers.put(reader);
                      }
                      processChunk(
                        series, resolution, plane, offset, shape, activeAxes);
                      LOGGER.info(
                          "Successfully processed chunk; resolution={} plane={}"
                          + " xx={} yy={} zz={} width={} height={} depth={}",
                          resolution, plane, xx, yy, zz, width, height, depth);
                      future.complete(null);
                    }
                    catch (Throwable t) {
                      future.completeExceptionally(t);
                      LOGGER.error(
                        "Failure processing chunk; resolution={} plane={} " +
                        "xx={} yy={} zz={} width={} height={} depth={}",
                        resolution, plane, xx, yy, zz, width, height, depth, t);
                    }
                  });
                }
              }
            }
          }
        }

        // Wait until the entire resolution has completed before proceeding to
        // the next one
        CompletableFuture.allOf(
          futures.toArray(new CompletableFuture[futures.size()])).join();

        // TODO: some of these futures may be completelyExceptionally
        //  and need re-throwing

      }
      finally {
        getProgressListener().notifyResolutionEnd(resolution);
      }
    }

    // series level metadata
    // do this after pixels are written, otherwise
    // min/max calculation won't have been performed
    setSeriesLevelMetadata(series, resolutions);
    getProgressListener().notifySeriesEnd(series);
  }

  private void saveHCSMetadata(IMetadata meta)
    throws IOException, ZarrException
  {
    if (noHCS) {
      LOGGER.debug("skipping HCS metadata");
      return;
    }
    LOGGER.debug("saving HCS metadata");

    // assumes only one plate defined
    int plate = 0;
    Map<String, Object> plateMap = new HashMap<String, Object>();

    plateMap.put("name", meta.getPlateName(plate));

    List<Map<String, Object>> columns = new ArrayList<Map<String, Object>>();
    List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();

    // try to set plate dimensions based upon Plate.Rows/Plate.Columns
    // if not possible, use well data
    PositiveInteger plateRows =  meta.getPlateRows(plate);
    if (plateRows == null) {
      plateRows = new PositiveInteger(1);
      for (int wellIndex=0; wellIndex<meta.getWellCount(plate); wellIndex++) {
        plateRows = new PositiveInteger(Math.max(
            meta.getWellRow(plate, wellIndex).getNumberValue().intValue(),
            plateRows.getNumberValue().intValue()));
      }
    }
    for (int r=0; r<plateRows.getValue(); r++) {
      Map<String, Object> row = new HashMap<String, Object>();
      String rowName = HCSIndex.getRowName(r);
      row.put("name", rowName);
      rows.add(row);
    }
    PositiveInteger plateColumns = meta.getPlateColumns(plate);
    if (plateColumns == null) {
      plateColumns = new PositiveInteger(1);
      for (int wellIndex=0; wellIndex<meta.getWellCount(plate); wellIndex++) {
        plateColumns = new PositiveInteger(Math.max(
            meta.getWellColumn(plate, wellIndex).getNumberValue().intValue(),
            plateColumns.getNumberValue().intValue()));
      }
    }
    for (int c=0; c<plateColumns.getValue(); c++) {
      Map<String, Object> column = new HashMap<String, Object>();
      String columnName = HCSIndex.getColumnName(c);
      column.put("name", columnName);
      columns.add(column);
    }

    List<Map<String, Object>> acquisitions =
      new ArrayList<Map<String, Object>>();
    for (int pa=0; pa<meta.getPlateAcquisitionCount(plate); pa++) {
      Map<String, Object> acquisition = new HashMap<String, Object>();
      acquisition.put("id", pa);
      acquisitions.add(acquisition);
    }
    if (acquisitions.size() > 0) {
      plateMap.put("acquisitions", acquisitions);
    }

    List<Map<String, Object>> wells = new ArrayList<Map<String, Object>>();
    int maxField = Integer.MIN_VALUE;
    for (HCSIndex index : hcsIndexes) {
      if (index.getPlateIndex() == plate) {
        if (index.getFieldIndex() == 0) {
          String wellPath = index.getWellPath();

          Map<String, Object> well = new HashMap<String, Object>();
          well.put("path", wellPath);

          List<Map<String, Object>> imageList =
            new ArrayList<Map<String, Object>>();
          for (HCSIndex field : hcsIndexes) {
            if (field.getPlateIndex() == index.getPlateIndex() &&
              field.getWellRowIndex() == index.getWellRowIndex() &&
              field.getWellColumnIndex() == index.getWellColumnIndex())
            {
              Map<String, Object> image = new HashMap<String, Object>();
              Integer plateAcq = field.getPlateAcquisitionIndex();
              if (plateAcq != null) {
                image.put("acquisition", plateAcq);
              }
              image.put("path", String.valueOf(field.getFieldIndex()));
              imageList.add(image);
            }
          }

          Map<String, Object> wellMap = new HashMap<String, Object>();
          wellMap.put("images", imageList);
          Map<String, Object> columnAttrs = new HashMap<String, Object>();
          columnAttrs.put("well", wellMap);

          String rowPath = index.getRowPath();
          String columnPath = index.getColumnPath();
          if (getV3()) {
            Group rowGroup = Group.create(v3Store.resolve(rowPath));
            Group columnGroup =
              Group.create(v3Store.resolve(rowPath, columnPath));
            Map<String, Object> omeAttrs = new HashMap<String, Object>();
            omeAttrs.put("ome", columnAttrs);
            columnGroup.setAttributes(omeAttrs);
          }
          else {
            Path rootPath = getRootPath();
            ZarrGroup root = ZarrGroup.open(rootPath);
            ZarrGroup rowGroup = root.createSubGroup(rowPath);
            ZarrGroup columnGroup = rowGroup.createSubGroup(columnPath);
            columnGroup.writeAttributes(columnAttrs);
          }

          // make sure the row/column indexes are added to the plate attributes
          // this is necessary when Plate.Rows or Plate.Columns is not set
          String column = index.getColumnPath();
          String row = index.getRowPath();

          int columnIndex = -1;
          for (int c=0; c<columns.size(); c++) {
            if (columns.get(c).get("name").equals(column)) {
              columnIndex = c;
              break;
            }
          }
          if (columnIndex < 0) {
            Map<String, Object> colMap = new HashMap<String, Object>();
            colMap.put("name", column);
            columnIndex = columns.size();
            columns.add(colMap);
          }

          int rowIndex = -1;
          for (int r=0; r<rows.size(); r++) {
            if (rows.get(r).get("name").equals(row)) {
              rowIndex = r;
              break;
            }
          }
          if (rowIndex < 0) {
            Map<String, Object> rowMap = new HashMap<String, Object>();
            rowMap.put("name", row);
            rowIndex = rows.size();
            rows.add(rowMap);
          }

          well.put("rowIndex", rowIndex);
          well.put("columnIndex", columnIndex);
          wells.add(well);
        }

        maxField = (int) Math.max(maxField, index.getFieldIndex());
      }
    }
    plateMap.put("wells", wells);
    plateMap.put("columns", columns);
    plateMap.put("rows", rows);

    plateMap.put("field_count", maxField + 1);
    plateMap.put("version", getNGFFVersion());

    if (getV3()) {
      Group v3Group = Group.open(v3Store.resolve());
      Map<String, Object> attributes = v3Group.metadata.attributes;
      Map<String, Object> omeAttributes = null;
      if (attributes.containsKey("ome")) {
        omeAttributes = (Map<String, Object>) attributes.get("ome");
      }
      else {
        omeAttributes = new HashMap<String, Object>();
      }
      omeAttributes.put("plate", plateMap);
      attributes.put("ome", omeAttributes);
      v3Group.setAttributes(attributes);
    }
    else {
      Path rootPath = getRootPath();
      ZarrGroup root = ZarrGroup.open(rootPath);
      Map<String, Object> attributes = root.getAttributes();
      attributes.put("plate", plateMap);
      root.writeAttributes(attributes);
    }
  }

  /**
   * Use {@link ZarrArray#writeAttributes(Map)}
   * to attach the multiscales metadata to the group containing
   * the pyramids.
   *
   * @param series Series which is currently being written.
   * @param resolutions Total number of resolutions from which
   *                    names will be generated.
   * @throws IOException
   * @throws InterruptedException
   */
  private void setSeriesLevelMetadata(int series, int resolutions)
      throws IOException, InterruptedException, EnumerationException,
        ZarrException
  {
    LOGGER.debug("setSeriesLevelMetadata({}, {})", series, resolutions);
    String resolutionString = String.format(
            scaleFormatString, getScaleFormatStringArgs(series, 0));
    if (resolutionString.endsWith("/")) {
      resolutionString = resolutionString.substring(
        0, resolutionString.length() - 1);
    }
    String seriesString = "";
    if (resolutionString.indexOf('/') >= 0) {
      seriesString = resolutionString.substring(0,
          resolutionString.lastIndexOf('/'));
    }
    LOGGER.debug("  seriesString = {}", seriesString);
    LOGGER.debug("  resolutionString = {}", resolutionString);
    List<Map<String, Object>> multiscales =
            new ArrayList<Map<String, Object>>();
    Map<String, Object> multiscale = new HashMap<String, Object>();
    Map<String, String> metadata = new HashMap<String, String>();

    if (downsampling == Downsampling.SIMPLE) {
      metadata.put("method", "loci.common.image.SimpleImageScaler");
      metadata.put("version", "Bio-Formats " + FormatTools.VERSION);
    }
    else {
      String method =
        downsampling == Downsampling.GAUSSIAN ? "pyrDown" : "resize";
      metadata.put("method", "org.opencv.imgproc.Imgproc." + method);
      metadata.put("version", OpenCVTools.getVersion());
      multiscale.put("type", downsampling.getName());
    }
    multiscale.put("metadata", metadata);
    if (!getV3()) {
      multiscale.put("version", getNGFFVersion());
    }
    multiscales.add(multiscale);

    IFormatReader v = null;
    IMetadata meta = null;
    List<Axis> activeAxes = null;

    try {
      v = readers.take();
      meta = (IMetadata) v.getMetadataStore();

      activeAxes = getDimensions(v, v.getSizeX(), v.getSizeY(),
        v.getSizeZ(), 1, 1, 1);
    }
    finally {
      readers.put(v);
    }

    List<Map<String, Object>> datasets = new ArrayList<Map<String, Object>>();
    for (int r = 0; r < resolutions; r++) {
      resolutionString = String.format(
              scaleFormatString, getScaleFormatStringArgs(series, r));

      // calculate the relative path to this resolution
      String lastPath = resolutionString;
      int lastFileSeparator = lastPath.lastIndexOf('/');
      if (lastFileSeparator == lastPath.length() - 1) {
        // if there is a trailing slash, remove it and recalculate
        // the last file separator index
        lastPath = lastPath.substring(0, lastFileSeparator);
        lastFileSeparator = lastPath.lastIndexOf('/');
      }
      lastPath = lastPath.substring(lastFileSeparator + 1);

      List<Map<String, Object>> transforms =
        new ArrayList<Map<String, Object>>();
      Map<String, Object> scale = new HashMap<String, Object>();
      scale.put("type", "scale");
      List<Double> axisValues = new ArrayList<Double>();
      double resolutionScale = Math.pow(PYRAMID_SCALE, r);
      for (int i=0; i<activeAxes.size(); i++) {
        String axisChar =
          String.valueOf(activeAxes.get(i).getType()).toLowerCase();
        Quantity axisScale = getScale(meta, series, axisChar.charAt(0));

        if (axisScale != null) {
          // if physical dimension information is defined,
          // use it directly for dimensions that aren't scaled (Z and T)
          // increase it according to the resolution number for dimensions that
          // are scaled (X and Y)
          if (axisChar.equals("x") || axisChar.equals("y")) {
            axisValues.add(axisScale.value().doubleValue() * resolutionScale);
          }
          else {
            axisValues.add(axisScale.value().doubleValue());
          }
        }
        else {
          // if physical dimension information is not defined,
          // store the scale factor for the dimension in the current resolution,
          // i.e. 1.0 for everything other than X and Y
          if (axisChar.equals("x") || axisChar.equals("y")) {
            axisValues.add(resolutionScale);
          }
          else {
            axisValues.add(1.0);
          }
        }
      }
      scale.put("scale", axisValues);

      transforms.add(scale);

      Map<String, Object> dataset = new HashMap<String, Object>();
      dataset.put("path", lastPath);
      dataset.put("coordinateTransformations", transforms);
      datasets.add(dataset);
    }
    multiscale.put("datasets", datasets);

    List<Map<String, String>> axes = new ArrayList<Map<String, String>>();
    for (int i=0; i<activeAxes.size(); i++) {
      String axis = String.valueOf(activeAxes.get(i).getType()).toLowerCase();
      String type = "space";
      Quantity scale = getScale(meta, series, axis.charAt(0));
      if (axis.equals("t")) {
        type = "time";
      }
      else if (axis.equals("c")) {
        type = "channel";
      }
      Map<String, String> thisAxis = new HashMap<String, String>();
      thisAxis.put("name", axis);
      thisAxis.put("type", type);
      if (scale != null) {
        String symbol = scale.unit().getSymbol();
        String unitName = null;
        try {
          if (scale instanceof Length) {
            unitName = UnitsLength.fromString(symbol).name().toLowerCase();
          }
          else if (scale instanceof Time) {
            unitName = UnitsTime.fromString(symbol).name().toLowerCase();
          }
        }
        catch (EnumerationException e) {
          LOGGER.warn("Could not identify unit '{}'", symbol);
        }
        thisAxis.put("unit", unitName);
      }
      axes.add(thisAxis);
    }
    multiscale.put("axes", axes);

    int seriesIndex = seriesList.indexOf(series);
    String name = meta.getImageName(seriesIndex);
    if (name == null) {
      name = "Series " + seriesIndex;
    }
    multiscale.put("name", name);

    Map<String, Object> attributes = new HashMap<String, Object>();
    Map<String, Object> omeAttributes = new HashMap<String, Object>();
    if (getV3()) {
      omeAttributes.put("version", getNGFFVersion());
      omeAttributes.put("multiscales", multiscales);
    }
    else {
      attributes.put("multiscales", multiscales);
    }

    if (omeroMetadata) {
      Map<String, Object> omero = new HashMap<String, Object>();

      int channelCount = meta.getChannelCount(seriesIndex);
      boolean colorRender = channelCount > 1 && channelCount < 8;

      Map<String, Object> rdefs = new HashMap<String, Object>();
      rdefs.put("defaultT", 0);
      rdefs.put("defaultZ", meta.getPixelsSizeZ(seriesIndex).getValue() / 2);
      rdefs.put("model", colorRender ? "color" : "greyscale");
      omero.put("rdefs", rdefs);

      double[] defaultMinMax =
        ZarrTypes.getRange(FormatTools.pixelTypeFromString(
          meta.getPixelsType(seriesIndex).toString()));

      OMEXMLMetadata omexml = (OMEXMLMetadata) meta;
      omexml.resolveReferences();

      List<Map<String, Object>> channels = new ArrayList<Map<String, Object>>();
      for (int c=0; c<channelCount; c++) {
        Map<String, Object> channel = new HashMap<String, Object>();
        channel.put("active", c < 3);
        channel.put("coefficient", 1);

        // set an RGB color (alpha removed)
        Color color = Colors.getColor(omexml, seriesIndex, c);
        Integer packedColor = (color.getValue() >> 8) & 0xffffff;
        String formattedColor = String.format("%06X", packedColor);
        channel.put("color", formattedColor);

        channel.put("family", "linear");
        channel.put("inverted", false);
        channel.put("label", getChannelName(omexml, seriesIndex, c));
        Map<String, Object> window = new HashMap<String, Object>();

        final int channelIndex = c;
        Double[] totalMin = new Double[1];
        Double[] totalMax = new Double[1];

        readers.forEach((minmax) -> {
          try {
            if (!(minmax instanceof MinMaxCalculator)) {
              LOGGER.error("Cannot set OMERO min/max data");
            }

            MinMaxCalculator calc = (MinMaxCalculator) minmax;
            Double min = calc.getChannelKnownMinimum(channelIndex);
            Double max = calc.getChannelKnownMaximum(channelIndex);

            if (min != null && min < Double.POSITIVE_INFINITY) {
              if (totalMin[0] == null || min < totalMin[0]) {
                totalMin[0] = min;
              }
            }
            if (max != null && max > Double.NEGATIVE_INFINITY) {
              if (totalMax[0] == null || max > totalMax[0]) {
                totalMax[0] = max;
              }
            }
          }
          catch (FormatException|IOException e) {
            LOGGER.error("Cannot set OMERO min/max data", e);
          }
        });

        if (totalMin[0] == null && defaultMinMax != null) {
          totalMin[0] = defaultMinMax[0];
        }
        if (totalMax[0] == null && defaultMinMax != null) {
          totalMax[0] = defaultMinMax[1];
        }

        window.put("start", totalMin[0]);
        window.put("end", totalMax[0]);
        window.put("min", totalMin[0]);
        window.put("max", totalMax[0]);
        channel.put("window", window);

        channels.add(channel);
      }

      omero.put("channels", channels);

      if (getV3()) {
        omeAttributes.put("omero", omero);
      }
      else {
        attributes.put("omero", omero);
      }
    }

    if (getV3()) {
      attributes.put("ome", omeAttributes);
      Group v3Group = Group.create(v3Store.resolve(seriesString));
      v3Group.setAttributes(attributes);
    }
    else {
      Path subGroupPath = getRootPath().resolve(seriesString);
      LOGGER.debug("  creating subgroup {}", subGroupPath);
      ZarrGroup subGroup = ZarrGroup.create(subGroupPath);
      subGroup.writeAttributes(attributes);
    }
    LOGGER.debug("    finished writing subgroup attributes");
  }

  private Quantity getScale(IMetadata meta, int series, char axisChar) {
    if (meta == null) {
      return null;
    }
    int seriesIndex = seriesList.indexOf(series);

    if (seriesIndex < 0 || seriesIndex >= meta.getImageCount()) {
      return null;
    }

    switch (axisChar) {
      case 'x':
        return meta.getPixelsPhysicalSizeX(seriesIndex);
      case 'y':
        return meta.getPixelsPhysicalSizeY(seriesIndex);
      case 'z':
        return meta.getPixelsPhysicalSizeZ(seriesIndex);
      case 't':
        Quantity timeIncrement = meta.getPixelsTimeIncrement(seriesIndex);
        if (timeIncrement != null && timeIncrement.value().doubleValue() > 0) {
          return timeIncrement;
        }
        else {
          return null;
        }
      default:
        return null;
    }
  }

  /**
   * Takes exception from asynchronous execution and re-throw known exception
   * types. If the end is reached with no known exception detected, either the
   * exception itself will be thrown if {@link RuntimeException}, otherwise
   * wrap in a {@link RuntimeException}.
   *
   * @param t Exception raised during processing.
   */
  private void unwrapException(Throwable t)
          throws FormatException, IOException, InterruptedException
  {
    if (t instanceof CompletionException) {
      try {
        throw ((CompletionException) t).getCause();
      }
      catch (FormatException | IOException | InterruptedException e2) {
        throw e2;
      }
      catch (RuntimeException rt) {
        throw rt;
      }
      catch (Throwable t2) {
        throw new RuntimeException(t);
      }
    }
    else if (t instanceof RuntimeException) {
      throw (RuntimeException) t;
    }
    else {
      throw new RuntimeException(t);
    }
  }

  private OMEXMLService getService() throws FormatException {
    try {
      ServiceFactory factory = new ServiceFactory();
      return factory.getInstance(OMEXMLService.class);
    }
    catch (DependencyException de) {
      throw new MissingLibraryException(OMEXMLServiceImpl.NO_OME_XML_MSG, de);
    }
  }

  /**
   * @return an empty IMetadata object for metadata transport.
   * @throws FormatException
   */
  private IMetadata createMetadata() throws FormatException {
    try {
      return getService().createOMEXMLMetadata();
    }
    catch (ServiceException se) {
      throw new FormatException(se);
    }
  }

  private DimensionSeparator getDimensionSeparator() {
    return nested ? DimensionSeparator.SLASH : DimensionSeparator.DOT;
  }

  private void checkOutputPaths() {
    if (pyramidName != null || !scaleFormatString.equals("%d/%d")) {
      LOGGER.info("Output will be incompatible with raw2ometiff " +
              "(pyramidName: {}, scaleFormatString: {})",
              pyramidName, scaleFormatString);
    }

    if (additionalScaleFormatStringArgsCsv != null) {
      CsvParserSettings parserSettings = new CsvParserSettings();
      parserSettings.detectFormatAutomatically();
      parserSettings.setLineSeparatorDetectionEnabled(true);

      CsvParser parser = new CsvParser(parserSettings);
      additionalScaleFormatStringArgs =
          parser.parseAll(additionalScaleFormatStringArgsCsv.toFile());
    }
  }

  private Class<?> getBaseReaderClass() throws FormatException, IOException {
    ClassList<IFormatReader> readerClasses =
        ImageReader.getDefaultReaderClasses();

    for (Class<?> reader : extraReaders) {
      readerClasses.addClass(0, (Class<IFormatReader>) reader);
      LOGGER.debug("Added extra reader: {}", reader);
    }

    ImageReader imageReader = new ImageReader(readerClasses);
    try {
      imageReader.setId(inputPath.toString());
      return imageReader.getReader().getClass();
    }
    finally {
      imageReader.close();
    }
  }

  /**
   * Check if the given metadata object contains at least one plate
   * with at least one well that links to an image.
   *
   * @param meta metadata object
   * @return true if a valid plate exists, false otherwise
   */
  private boolean hasValidPlate(IMetadata meta) {
    List<String> images = new ArrayList<String>();
    for (int i=0; i<meta.getImageCount(); i++) {
      images.add(meta.getImageID(i));
    }
    for (int p=0; p<meta.getPlateCount(); p++) {
      for (int w=0; w<meta.getWellCount(p); w++) {
        for (int ws=0; ws<meta.getWellSampleCount(p, w); ws++) {
          if (images.contains(meta.getWellSampleImageRef(p, w, ws))) {
            return true;
          }
        }
      }
      LOGGER.warn("Encountered invalid plate #{}", p);
    }
    return false;
  }

  private String getChannelName(OMEXMLMetadata meta,
    int series, int channelIndex)
  {
    String channelName = meta.getChannelName(series, channelIndex);
    if (channelName != null) {
      return channelName;
    }

    Length emission = meta.getChannelEmissionWavelength(series, channelIndex);
    Length excitation =
      meta.getChannelExcitationWavelength(series, channelIndex);

    OMEXMLMetadataRoot root = (OMEXMLMetadataRoot) meta.getRoot();
    Image img = root.getImage(series);
    Channel channel = img.getPixels().getChannel(channelIndex);
    LightPath path = channel.getLightPath();
    FilterSet filterSet = channel.getLinkedFilterSet();
    LightSourceSettings sourceSettings = channel.getLightSourceSettings();
    LightSource source = null;
    if (sourceSettings != null) {
      source = sourceSettings.getLightSource();
    }

    if (emission != null) {
      String name = getNameFromWavelength(emission);
      if (name != null) {
        return name;
      }
    }

    if (path != null) {
      List<Filter> emFilters = path.copyLinkedEmissionFilterList();
      for (Filter f : emFilters) {
        String name = getNameFromFilter(f);
        if (name != null) {
          return name;
        }
      }
    }

    if (filterSet != null) {
      List<Filter> emFilters = filterSet.copyLinkedEmissionFilterList();
      for (Filter f : emFilters) {
        String name = getNameFromFilter(f);
        if (name != null) {
          return name;
        }
      }
    }
    if (source != null && source instanceof Laser) {
      Laser laser = (Laser) source;
      if (laser.getWavelength() != null) {
        String name = getNameFromWavelength(laser.getWavelength());
        if (name != null) {
          return name;
        }
      }
    }
    if (excitation != null) {
      String name = getNameFromWavelength(excitation);
      if (name != null) {
        return name;
      }
    }

    if (path != null) {
      List<Filter> exFilters = path.copyLinkedExcitationFilterList();
      for (Filter f : exFilters) {
        String name = getNameFromFilter(f);
        if (name != null) {
          return name;
        }
      }
    }
    if (filterSet != null) {
      List<Filter> exFilters = filterSet.copyLinkedExcitationFilterList();
      for (Filter f : exFilters) {
        String name = getNameFromFilter(f);
        if (name != null) {
          return name;
        }
      }
    }
    return "Channel " + channelIndex;
  }

  private String getNameFromFilter(Filter f) {
    if (f == null) {
      return null;
    }
    TransmittanceRange range = f.getTransmittanceRange();
    if (range == null) {
      return null;
    }
    return String.valueOf(range.getCutIn().value());
  }

  private String getNameFromWavelength(Length v) {
    if (v == null) {
      return null;
    }
    double wave = v.value().doubleValue();
    if (DoubleMath.isMathematicalInteger(wave)) {
      return String.valueOf(wave);
    }
    return v.toString();
  }

  private int calculateResolutions(int width, int height) {
    int resolutions = 1;
    while (width > minSize || height > minSize) {
      resolutions++;
      width /= PYRAMID_SCALE;
      height /= PYRAMID_SCALE;
    }
    return resolutions;
  }

  private String getNGFFVersion() {
    if (getV3()) {
      return NGFF_VERSION_V3;
    }
    return getNested() ? NGFF_VERSION : "0.1";
  }

  /**
   * Check that the desired chunk and shard sizes are compatible.
   * In each dimension, the chunk size must evenly divide into the shard size.
   *
   * @param chunkSize expected chunk size
   * @param shardSize expected shard size
   * @param shape array shape
   * @return true if the chunk and shard can be used together
   */
  private boolean chunkAndShardCompatible(
    int[] chunkSize, int[] shardSize, int[] shape)
  {
    if (chunkSize.length != shardSize.length ||
      shape.length != chunkSize.length)
    {
      return false;
    }
    for (int d=0; d<shape.length; d++) {
      if (shardSize[d] % chunkSize[d] != 0) {
        LOGGER.warn("Shard={} not compatible with chunk={} (axis {})",
          shardSize[d], chunkSize[d], d);
        return false;
      }
      if (chunkSize[d] > shape[d]) {
        LOGGER.warn("Shard={} must be smaller than shape={} (axis {})",
          shardSize[d], shape[d], d);
        return false;
      }
    }
    return true;
  }

  /**
   * Convert the specified compression type into a v3 CodecBuilder.
   * This builder can then be applied to either the chunk or the shard.
   * If the specified compression type is not supported yet, a warning
   * is logged and the CodecBuilder will not include any compression.
   *
   * @param builder non-null CodecBuilder
   * @return CodecBuilder with compression applied
   */
  private CodecBuilder applyCompressionType(CodecBuilder builder) {
    if (getCompression() == ZarrCompression.blosc) {
      return builder.withBlosc();
    }
    else if (getCompression() != ZarrCompression.raw) {
      LOGGER.warn("Skipping unsupported compression: {}", getCompression());
    }
    return builder;
  }

  private static Slf4JStopWatch stopWatch() {
    return new Slf4JStopWatch(LOGGER, Slf4JStopWatch.DEBUG_LEVEL);
  }

  /**
   * Set a listener for tile processing events.
   * Intended to be used to show a status bar.
   *
   * @param listener a progress event listener
   */
  public void setProgressListener(IProgressListener listener) {
    progressListener = listener;
  }

  /**
   * Get the currrent listener for tile processing events.
   * If no listener was set, a no-op listener is returned.
   *
   * @return the current progress listener
   */
  public IProgressListener getProgressListener() {
    if (progressListener == null) {
      setProgressListener(new NoOpProgressListener());
    }
    return progressListener;
  }

  /**
   * Perform file conversion as specified by command line arguments.
   * @param args command line arguments
   */
  public static void main(String[] args) {
    int exitCode = new CommandLine(new Converter()).execute(args);
    System.exit(exitCode);
  }

}
